{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0a00752",
   "metadata": {
    "id": "KuNrQK8Y6epJ",
    "papermill": {
     "duration": 0.006679,
     "end_time": "2024-12-30T15:22:29.117792",
     "exception": false,
     "start_time": "2024-12-30T15:22:29.111113",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99cf4b77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T15:22:29.131149Z",
     "iopub.status.busy": "2024-12-30T15:22:29.130510Z",
     "iopub.status.idle": "2024-12-30T15:22:32.046773Z",
     "shell.execute_reply": "2024-12-30T15:22:32.045870Z"
    },
    "id": "W9yu9M1T6epN",
    "papermill": {
     "duration": 2.925027,
     "end_time": "2024-12-30T15:22:32.048799",
     "exception": false,
     "start_time": "2024-12-30T15:22:29.123772",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import  json\n",
    "from enum import Enum\n",
    "\n",
    "class ConfigEncoder(json.JSONEncoder):\n",
    "    def default(self, o):\n",
    "        if isinstance(o, type):\n",
    "            return {'$class': o.__module__ + \".\" + o.__name__}\n",
    "        elif isinstance(o, Enum):\n",
    "            return {\n",
    "                '$enum': o.__module__ + \".\" + o.__class__.__name__ + '.' + o.name\n",
    "            }\n",
    "        elif callable(o):\n",
    "            return {\n",
    "                '$function': o.__module__ + \".\" + o.__name__\n",
    "            }\n",
    "        return json.JSONEncoder.default(self, o)\n",
    "\n",
    "def count_parameters(model, trainable=False):\n",
    "    if trainable:\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "\n",
    "def tensor2numpy(x):\n",
    "    return x.cpu().data.numpy() if x.is_cuda else x.data.numpy()\n",
    "\n",
    "\n",
    "def target2onehot(targets, n_classes):\n",
    "    onehot = torch.zeros(targets.shape[0], n_classes).to(targets.device)\n",
    "    onehot.scatter_(dim=1, index=targets.long().view(-1, 1), value=1.0)\n",
    "    return onehot\n",
    "\n",
    "\n",
    "def makedirs(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "\n",
    "def accuracy(y_pred, y_true, nb_old, increment=10):\n",
    "    assert len(y_pred) == len(y_true), \"Data length error.\"\n",
    "    all_acc = {}\n",
    "    all_acc[\"total\"] = np.around(\n",
    "        (y_pred == y_true).sum() * 100 / len(y_true), decimals=2\n",
    "    )\n",
    "\n",
    "    # Grouped accuracy\n",
    "    for class_id in range(0, np.max(y_true), increment):\n",
    "        idxes = np.where(\n",
    "            np.logical_and(y_true >= class_id, y_true < class_id + increment)\n",
    "        )[0]\n",
    "        label = \"{}-{}\".format(\n",
    "            str(class_id).rjust(2, \"0\"), str(class_id + increment - 1).rjust(2, \"0\")\n",
    "        )\n",
    "        all_acc[label] = np.around(\n",
    "            (y_pred[idxes] == y_true[idxes]).sum() * 100 / len(idxes), decimals=2\n",
    "        )\n",
    "\n",
    "    # Old accuracy\n",
    "    idxes = np.where(y_true < nb_old)[0]\n",
    "    all_acc[\"old\"] = (\n",
    "        0\n",
    "        if len(idxes) == 0\n",
    "        else np.around(\n",
    "            (y_pred[idxes] == y_true[idxes]).sum() * 100 / len(idxes), decimals=2\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # New accuracy\n",
    "    idxes = np.where(y_true >= nb_old)[0]\n",
    "    all_acc[\"new\"] = np.around(\n",
    "        (y_pred[idxes] == y_true[idxes]).sum() * 100 / len(idxes), decimals=2\n",
    "    )\n",
    "\n",
    "    return all_acc\n",
    "\n",
    "\n",
    "def split_images_labels(imgs):\n",
    "    # split trainset.imgs in ImageFolder\n",
    "    images = []\n",
    "    labels = []\n",
    "    for item in imgs:\n",
    "        images.append(item[0])\n",
    "        labels.append(item[1])\n",
    "\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "def save_fc(args, model):\n",
    "    _path = os.path.join(args['logfilename'], \"fc.pt\")\n",
    "    if len(args['device']) > 1:\n",
    "        fc_weight = model._network.fc.weight.data\n",
    "    else:\n",
    "        fc_weight = model._network.fc.weight.data.cpu()\n",
    "    torch.save(fc_weight, _path)\n",
    "\n",
    "    _save_dir = os.path.join(f\"./results/fc_weights/{args['prefix']}\")\n",
    "    os.makedirs(_save_dir, exist_ok=True)\n",
    "    _save_path = os.path.join(_save_dir, f\"{args['csv_name']}.csv\")\n",
    "    with open(_save_path, \"a+\") as f:\n",
    "        f.write(f\"{args['time_str']},{args['model_name']},{_path} \\n\")\n",
    "\n",
    "def save_model(args, model):\n",
    "    #used in PODNet\n",
    "    _path = os.path.join(args['logfilename'], \"model.pt\")\n",
    "    if len(args['device']) > 1:\n",
    "        weight = model._network\n",
    "    else:\n",
    "        weight = model._network.cpu()\n",
    "    torch.save(weight, _path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24778a7a",
   "metadata": {
    "id": "AiA7LXtv6epQ",
    "papermill": {
     "duration": 0.006105,
     "end_time": "2024-12-30T15:22:32.060926",
     "exception": false,
     "start_time": "2024-12-30T15:22:32.054821",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# KAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17e6527b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T15:22:32.075170Z",
     "iopub.status.busy": "2024-12-30T15:22:32.074788Z",
     "iopub.status.idle": "2024-12-30T15:22:32.103070Z",
     "shell.execute_reply": "2024-12-30T15:22:32.102223Z"
    },
    "id": "wjyYT2Kf6epR",
    "papermill": {
     "duration": 0.036989,
     "end_time": "2024-12-30T15:22:32.104666",
     "exception": false,
     "start_time": "2024-12-30T15:22:32.067677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "################## Modified the output format for KANLinear so working of KAN is implacted ##############3\n",
    "\n",
    "class KANLinear(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        out_features,\n",
    "        grid_size=5,\n",
    "        spline_order=3,\n",
    "        scale_noise=0.1,\n",
    "        scale_base=1.0,\n",
    "        scale_spline=1.0,\n",
    "        enable_standalone_scale_spline=True,\n",
    "        base_activation=torch.nn.SiLU,\n",
    "        grid_eps=0.02,\n",
    "        grid_range=[-1, 1],\n",
    "    ):\n",
    "        super(KANLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.grid_size = grid_size\n",
    "        self.spline_order = spline_order\n",
    "\n",
    "        h = (grid_range[1] - grid_range[0]) / grid_size\n",
    "        grid = (\n",
    "            (\n",
    "                torch.arange(-spline_order, grid_size + spline_order + 1) * h\n",
    "                + grid_range[0]\n",
    "            )\n",
    "            .expand(in_features, -1)\n",
    "            .contiguous()\n",
    "        )\n",
    "        self.register_buffer(\"grid\", grid)\n",
    "\n",
    "        self.base_weight = torch.nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.spline_weight = torch.nn.Parameter(\n",
    "            torch.Tensor(out_features, in_features, grid_size + spline_order)\n",
    "        )\n",
    "        if enable_standalone_scale_spline:\n",
    "            self.spline_scaler = torch.nn.Parameter(\n",
    "                torch.Tensor(out_features, in_features)\n",
    "            )\n",
    "\n",
    "        self.scale_noise = scale_noise\n",
    "        self.scale_base = scale_base\n",
    "        self.scale_spline = scale_spline\n",
    "        self.enable_standalone_scale_spline = enable_standalone_scale_spline\n",
    "        self.base_activation = base_activation()\n",
    "        self.grid_eps = grid_eps\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.kaiming_uniform_(self.base_weight, a=math.sqrt(5) * self.scale_base)\n",
    "        with torch.no_grad():\n",
    "            noise = (\n",
    "                (\n",
    "                    torch.rand(self.grid_size + 1, self.in_features, self.out_features)\n",
    "                    - 1 / 2\n",
    "                )\n",
    "                * self.scale_noise\n",
    "                / self.grid_size\n",
    "            )\n",
    "            self.spline_weight.data.copy_(\n",
    "                (self.scale_spline if not self.enable_standalone_scale_spline else 1.0)\n",
    "                * self.curve2coeff(\n",
    "                    self.grid.T[self.spline_order : -self.spline_order],\n",
    "                    noise,\n",
    "                )\n",
    "            )\n",
    "            if self.enable_standalone_scale_spline:\n",
    "                # torch.nn.init.constant_(self.spline_scaler, self.scale_spline)\n",
    "                torch.nn.init.kaiming_uniform_(self.spline_scaler, a=math.sqrt(5) * self.scale_spline)\n",
    "\n",
    "    def b_splines(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Compute the B-spline bases for the given input tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: B-spline bases tensor of shape (batch_size, in_features, grid_size + spline_order).\n",
    "        \"\"\"\n",
    "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
    "\n",
    "        grid: torch.Tensor = (\n",
    "            self.grid\n",
    "        )  # (in_features, grid_size + 2 * spline_order + 1)\n",
    "        x = x.unsqueeze(-1)\n",
    "        bases = ((x >= grid[:, :-1]) & (x < grid[:, 1:])).to(x.dtype)\n",
    "        for k in range(1, self.spline_order + 1):\n",
    "            bases = (\n",
    "                (x - grid[:, : -(k + 1)])\n",
    "                / (grid[:, k:-1] - grid[:, : -(k + 1)])\n",
    "                * bases[:, :, :-1]\n",
    "            ) + (\n",
    "                (grid[:, k + 1 :] - x)\n",
    "                / (grid[:, k + 1 :] - grid[:, 1:(-k)])\n",
    "                * bases[:, :, 1:]\n",
    "            )\n",
    "\n",
    "        assert bases.size() == (\n",
    "            x.size(0),\n",
    "            self.in_features,\n",
    "            self.grid_size + self.spline_order,\n",
    "        )\n",
    "        return bases.contiguous()\n",
    "\n",
    "    def curve2coeff(self, x: torch.Tensor, y: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Compute the coefficients of the curve that interpolates the given points.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n",
    "            y (torch.Tensor): Output tensor of shape (batch_size, in_features, out_features).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Coefficients tensor of shape (out_features, in_features, grid_size + spline_order).\n",
    "        \"\"\"\n",
    "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
    "        assert y.size() == (x.size(0), self.in_features, self.out_features)\n",
    "\n",
    "        A = self.b_splines(x).transpose(\n",
    "            0, 1\n",
    "        )  # (in_features, batch_size, grid_size + spline_order)\n",
    "        B = y.transpose(0, 1)  # (in_features, batch_size, out_features)\n",
    "        solution = torch.linalg.lstsq(\n",
    "            A, B\n",
    "        ).solution  # (in_features, grid_size + spline_order, out_features)\n",
    "        result = solution.permute(\n",
    "            2, 0, 1\n",
    "        )  # (out_features, in_features, grid_size + spline_order)\n",
    "\n",
    "        assert result.size() == (\n",
    "            self.out_features,\n",
    "            self.in_features,\n",
    "            self.grid_size + self.spline_order,\n",
    "        )\n",
    "        return result.contiguous()\n",
    "\n",
    "    @property\n",
    "    def scaled_spline_weight(self):\n",
    "        return self.spline_weight * (\n",
    "            self.spline_scaler.unsqueeze(-1)\n",
    "            if self.enable_standalone_scale_spline\n",
    "            else 1.0\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if isinstance(x, dict):   # needed cuz i modified format of output\n",
    "            x = x['logits']\n",
    "        # print(\"Assertion check --> \",x.size(-1),\" ---- \", self.in_features)\n",
    "        assert x.size(-1) == self.in_features\n",
    "        original_shape = x.shape\n",
    "        x = x.view(-1, self.in_features)\n",
    "\n",
    "        base_output = F.linear(self.base_activation(x), self.base_weight)\n",
    "        spline_output = F.linear(\n",
    "            self.b_splines(x).view(x.size(0), -1),\n",
    "            self.scaled_spline_weight.view(self.out_features, -1),\n",
    "        )\n",
    "        output = base_output + spline_output\n",
    "\n",
    "        output = output.view(*original_shape[:-1], self.out_features)\n",
    "        return {'logits': output}\n",
    "        # return output\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_grid(self, x: torch.Tensor, margin=0.01):\n",
    "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
    "        batch = x.size(0)\n",
    "\n",
    "        splines = self.b_splines(x)  # (batch, in, coeff)\n",
    "        splines = splines.permute(1, 0, 2)  # (in, batch, coeff)\n",
    "        orig_coeff = self.scaled_spline_weight  # (out, in, coeff)\n",
    "        orig_coeff = orig_coeff.permute(1, 2, 0)  # (in, coeff, out)\n",
    "        unreduced_spline_output = torch.bmm(splines, orig_coeff)  # (in, batch, out)\n",
    "        unreduced_spline_output = unreduced_spline_output.permute(\n",
    "            1, 0, 2\n",
    "        )  # (batch, in, out)\n",
    "\n",
    "        # sort each channel individually to collect data distribution\n",
    "        x_sorted = torch.sort(x, dim=0)[0]\n",
    "        grid_adaptive = x_sorted[\n",
    "            torch.linspace(\n",
    "                0, batch - 1, self.grid_size + 1, dtype=torch.int64, device=x.device\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        uniform_step = (x_sorted[-1] - x_sorted[0] + 2 * margin) / self.grid_size\n",
    "        grid_uniform = (\n",
    "            torch.arange(\n",
    "                self.grid_size + 1, dtype=torch.float32, device=x.device\n",
    "            ).unsqueeze(1)\n",
    "            * uniform_step\n",
    "            + x_sorted[0]\n",
    "            - margin\n",
    "        )\n",
    "\n",
    "        grid = self.grid_eps * grid_uniform + (1 - self.grid_eps) * grid_adaptive\n",
    "        grid = torch.concatenate(\n",
    "            [\n",
    "                grid[:1]\n",
    "                - uniform_step\n",
    "                * torch.arange(self.spline_order, 0, -1, device=x.device).unsqueeze(1),\n",
    "                grid,\n",
    "                grid[-1:]\n",
    "                + uniform_step\n",
    "                * torch.arange(1, self.spline_order + 1, device=x.device).unsqueeze(1),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        self.grid.copy_(grid.T)\n",
    "        self.spline_weight.data.copy_(self.curve2coeff(x, unreduced_spline_output))\n",
    "\n",
    "    def regularization_loss(self, regularize_activation=1.0, regularize_entropy=1.0):\n",
    "        \"\"\"\n",
    "        Compute the regularization loss.\n",
    "\n",
    "        This is a dumb simulation of the original L1 regularization as stated in the\n",
    "        paper, since the original one requires computing absolutes and entropy from the\n",
    "        expanded (batch, in_features, out_features) intermediate tensor, which is hidden\n",
    "        behind the F.linear function if we want an memory efficient implementation.\n",
    "\n",
    "        The L1 regularization is now computed as mean absolute value of the spline\n",
    "        weights. The authors implementation also includes this term in addition to the\n",
    "        sample-based regularization.\n",
    "        \"\"\"\n",
    "        l1_fake = self.spline_weight.abs().mean(-1)\n",
    "        regularization_loss_activation = l1_fake.sum()\n",
    "        p = l1_fake / regularization_loss_activation\n",
    "        regularization_loss_entropy = -torch.sum(p * p.log())\n",
    "        return (\n",
    "            regularize_activation * regularization_loss_activation\n",
    "            + regularize_entropy * regularization_loss_entropy\n",
    "        )\n",
    "\n",
    "\n",
    "class KAN(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        layers_hidden,\n",
    "        grid_size=5,\n",
    "        spline_order=3,\n",
    "        scale_noise=0.1,\n",
    "        scale_base=1.0,\n",
    "        scale_spline=1.0,\n",
    "        base_activation=torch.nn.SiLU,\n",
    "        grid_eps=0.02,\n",
    "        grid_range=[-1, 1],\n",
    "    ):\n",
    "        super(KAN, self).__init__()\n",
    "        self.grid_size = grid_size\n",
    "        self.spline_order = spline_order\n",
    "\n",
    "        self.out_features = layers_hidden[-1]\n",
    "\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for in_features, out_features in zip(layers_hidden, layers_hidden[1:]):\n",
    "            self.layers.append(\n",
    "                KANLinear(\n",
    "                    in_features,\n",
    "                    out_features,\n",
    "                    grid_size=grid_size,\n",
    "                    spline_order=spline_order,\n",
    "                    scale_noise=scale_noise,\n",
    "                    scale_base=scale_base,\n",
    "                    scale_spline=scale_spline,\n",
    "                    base_activation=base_activation,\n",
    "                    grid_eps=grid_eps,\n",
    "                    grid_range=grid_range,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, update_grid=False):\n",
    "        for layer in self.layers:\n",
    "            if update_grid:\n",
    "                layer.update_grid(x)\n",
    "            x = layer(x)\n",
    "        return x\n",
    "        # return {'logits': x}\n",
    "\n",
    "    def regularization_loss(self, regularize_activation=1.0, regularize_entropy=1.0):\n",
    "        return sum(\n",
    "            layer.regularization_loss(regularize_activation, regularize_entropy)\n",
    "            for layer in self.layers\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d257ce5",
   "metadata": {
    "id": "TMj_N-KG6epS",
    "papermill": {
     "duration": 0.0056,
     "end_time": "2024-12-30T15:22:32.115877",
     "exception": false,
     "start_time": "2024-12-30T15:22:32.110277",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Conv Nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a8a0206",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T15:22:32.129370Z",
     "iopub.status.busy": "2024-12-30T15:22:32.128842Z",
     "iopub.status.idle": "2024-12-30T15:22:33.521016Z",
     "shell.execute_reply": "2024-12-30T15:22:33.520165Z"
    },
    "id": "_Y1-sAOl6epS",
    "outputId": "297e7832-0f1a-4492-ebb2-994c948532e5",
    "papermill": {
     "duration": 1.40147,
     "end_time": "2024-12-30T15:22:33.523104",
     "exception": false,
     "start_time": "2024-12-30T15:22:32.121634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Reference:\n",
    "https://github.com/khurramjaved96/incremental-learning/blob/autoencoders/model/resnet32.py\n",
    "'''\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import VGG16_Weights\n",
    "from torchvision.models import alexnet, vgg16\n",
    "\n",
    "class Cifar_CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A basic CNN architecture for CIFAR-10, designed to replace the ResNet while keeping other components compatible.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels=3, num_classes=10):\n",
    "        super(Cifar_CNN, self).__init__()\n",
    "\n",
    "        self.conv_1_3x3 = nn.Conv2d(channels, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn_1 = nn.BatchNorm2d(16)\n",
    "\n",
    "        self.stage_1 = self._make_layer(16, 16, 1)\n",
    "        self.stage_2 = self._make_layer(16, 32, 2)\n",
    "        self.stage_3 = self._make_layer(32, 64, 2)\n",
    "\n",
    "\n",
    "        self.avgpool = nn.AvgPool2d(8)\n",
    "        self.out_dim = 64  # Matching the output dimension of ResNet's final block\n",
    "        self.fc = nn.Linear(self.out_dim, num_classes)\n",
    "\n",
    "        self.lin1 = self._make_linear(64,128)\n",
    "        self.lin2 = self._make_linear(128,64)\n",
    "\n",
    "        self.kan1 = KANLinear(64, 128)\n",
    "        self.kan2 = KANLinear(128,64)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _make_layer(self, in_channels, out_channels, stride):\n",
    "        layers = []\n",
    "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False))\n",
    "        layers.append(nn.BatchNorm2d(out_channels))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        layers.append(nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False))\n",
    "        layers.append(nn.BatchNorm2d(out_channels))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _make_linear(self,in_dims,out_dims):\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(in_dims,out_dims))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='linear')\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_1_3x3(x)  # [bs, 16, 32, 32]\n",
    "        x = F.relu(self.bn_1(x), inplace=True)\n",
    "\n",
    "        x_1 = self.stage_1(x)  # [bs, 16, 32, 32]\n",
    "        x_2 = self.stage_2(x_1)  # [bs, 32, 16, 16]\n",
    "        x_3 = self.stage_3(x_2)  # [bs, 64, 8, 8]\n",
    "\n",
    "        pooled = self.avgpool(x_3)  # [bs, 64, 1, 1]\n",
    "        features = pooled.view(pooled.size(0), -1)  # [bs, 64]\n",
    "        # features = self.lin1(features)  # MLP 1\n",
    "        # features = self.lin2(features)  # MLP 2\n",
    "        features = self.kan1(features)  # KAN 1\n",
    "        features = self.kan2(features)  # KAN 2\n",
    "\n",
    "        return {\n",
    "            'fmaps': [x_1, x_2, x_3],\n",
    "            'features': features\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def last_conv(self):\n",
    "        return self.stage_3[-1][0]\n",
    "\n",
    "# Define the VGG configuration\n",
    "VGG_CONFIGS = {\n",
    "    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'A-LRN': [64, 'LRN', 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'C': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, config='A', num_classes=100):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = self._make_layers(VGG_CONFIGS[config])\n",
    "\n",
    "        self.out_dim = 4096\n",
    "\n",
    "        self.avgpool = nn.AvgPool2d(3)\n",
    "        self.fc = nn.Linear(self.out_dim, num_classes)\n",
    "\n",
    "        # self.lin1 = self._make_linear(512, 4096)\n",
    "        self.lin1 = KANLinear(512,4096)\n",
    "        # self.lin2 = self._make_linear(4096, 4096)\n",
    "        self.lin2 = KANLinear(4096,4096)\n",
    "        self.lin3 = self._make_linear(4096, num_classes)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"Init. Shape is this ----->\", x.shape)\n",
    "\n",
    "        # Get feature maps from the original input\n",
    "        feature_maps = self.get_feature_maps(x)\n",
    "\n",
    "        x = self.features(x)\n",
    "        # print(\"After blocks Shape is this ----->\", x.shape)\n",
    "        x = self.avgpool(x)\n",
    "        # print(\"After Avgpool Shape is this ----->\", x.shape)\n",
    "        features = x.view(x.size(0), -1)  # Flatten\n",
    "        # print(\"Entering fc ----->\", features.shape)\n",
    "\n",
    "        # features = self.lin1(features)  # MLP 1\n",
    "        features = self.lin1(features)['logits']  # KAN 1\n",
    "        # print(\"Shape after fc1 --->\",features.shape)\n",
    "        # features = self.lin2(features)  # MLP 2\n",
    "        features = self.lin2(features)['logits']  # KAN 2\n",
    "        # print(\"Shape after fc2 --->\",features.shape)\n",
    "        # features = self.lin3(features)  # MLP 3\n",
    "\n",
    "        # print(\"Exited fc --->\", features.shape)\n",
    "        # print(\"inside arch:\",features.shape)\n",
    "\n",
    "        return {\n",
    "            'fmaps': feature_maps,\n",
    "            'features': features\n",
    "        }\n",
    "\n",
    "    def _make_layers(self, config):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in config:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            elif x == 'LRN':\n",
    "                layers += [nn.LocalResponseNorm(2)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1, bias=False),\n",
    "                           nn.BatchNorm2d(x),  # Adding BatchNorm as in Cifar_CNN\n",
    "                           nn.ReLU(inplace=True)]\n",
    "                in_channels = x\n",
    "        layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _make_linear(self, in_dims, out_dims):\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(in_dims, out_dims))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def get_feature_maps(self, x):\n",
    "        # print(\"enter\")\n",
    "        feature_maps = []\n",
    "        for name, layer in self.features._modules.items():\n",
    "            x = layer(x)\n",
    "            if isinstance(layer, nn.MaxPool2d):\n",
    "                feature_maps.append(x)\n",
    "        # print(\"test\")\n",
    "        return feature_maps\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='linear')\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    @property\n",
    "    def last_conv(self):\n",
    "        return self.features[-2]\n",
    "\n",
    "class VGGPretrained(nn.Module):\n",
    "    def __init__(self, num_classes=100, pretrained=True):\n",
    "        super(VGGPretrained, self).__init__()\n",
    "\n",
    "        # Load VGG16 architecture\n",
    "        self.vgg = vgg16(pretrained=False)\n",
    "        \n",
    "        # Load weights from the specific path if pretrained is True\n",
    "        if pretrained:\n",
    "            weights_path = \"/kaggle/input/vgg-imagenet/vgg16-397923af.pth\"\n",
    "            state_dict = torch.load(weights_path)\n",
    "            self.vgg.load_state_dict(state_dict)\n",
    "\n",
    "        # Extract VGG16's feature extractor and classifier components\n",
    "        self.features = self.vgg.features\n",
    "        self.out_dim = 4096\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))  # Adapts to variable input sizes for VGG\n",
    "        self.fc = nn.Linear(self.out_dim, num_classes)\n",
    "\n",
    "        # Define additional linear layers\n",
    "        self.lin1 = self._make_linear(512 * 7 * 7, 4096)  # Input size after avgpooling for VGG16\n",
    "        # self.lin2 = self._make_linear(4096, 4096) # MLP 2\n",
    "        self.lin2 = KANLinear(4096, 4096) # KAN 2\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Get feature maps\n",
    "        feature_maps = self.get_feature_maps(x)\n",
    "        \n",
    "        # Extract features using VGG's pre-trained layers\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        features = torch.flatten(x, 1)  # Flatten before passing to fully connected layers\n",
    "\n",
    "        # Pass through the fully connected layers\n",
    "        features = self.lin1(features)\n",
    "        # features = self.lin2(features) # MLP\n",
    "        features = self.lin2(features)['logits'] #KAN\n",
    "\n",
    "        return {\n",
    "            'fmaps': feature_maps,\n",
    "            'features': features\n",
    "        }\n",
    "\n",
    "    def _make_linear(self, in_dims, out_dims):\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(in_dims, out_dims))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def get_feature_maps(self, x):\n",
    "        # Get intermediate feature maps after each MaxPool2d layer in VGG16\n",
    "        feature_maps = []\n",
    "        for name, layer in self.features._modules.items():\n",
    "            x = layer(x)\n",
    "            if isinstance(layer, nn.MaxPool2d):\n",
    "                feature_maps.append(x)\n",
    "        return feature_maps\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='linear')\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    @property\n",
    "    def last_conv(self):\n",
    "        # Return the last convolutional layer in VGG16's feature extractor\n",
    "        return self.features[-2]\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "\n",
    "    def __init__(self, channels=3, num_classes=10):\n",
    "        super(AlexNet, self).__init__()\n",
    "\n",
    "        self.stage_1 = self._make_layer(3, 96, kernel_size=11,stride=2,padding=0)\n",
    "        self.stage_2 = self._make_layer(96, 256, kernel_size=5,padding=2)\n",
    "        self.stage_3 = self._make_layer(256,384,3,padding=1,simple=True,stride=2)\n",
    "        self.stage_4 = self._make_layer(384,384,3,padding=1,simple=True)\n",
    "        self.stage_5 = self._make_layer(384,256,3,padding=1,simple=True)\n",
    "\n",
    "        self.out_dim = 256*13*13\n",
    "        self.fc = nn.Linear(self.out_dim, num_classes)\n",
    "\n",
    "        self.lin1 = self._make_linear(256*13*13,4096)\n",
    "        self.lin2 = self._make_linear(4096,4096)\n",
    "\n",
    "        self.kan1 = KANLinear(256*13*13, 4096)\n",
    "        self.kan2 = KANLinear(4096,4096)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _make_layer(self, in_channels, out_channels, kernel_size,stride=1,padding=0,simple=False):\n",
    "        layers = []\n",
    "\n",
    "        if simple:\n",
    "            layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding,stride=stride))\n",
    "            layers.append(nn.ReLU(inplace=False))\n",
    "            return nn.Sequential(*layers)\n",
    "\n",
    "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding))\n",
    "        layers.append(nn.ReLU(inplace=False))\n",
    "        layers.append(nn.LocalResponseNorm(size=5,alpha=0.0001,beta=0.75,k=2))\n",
    "        layers.append(nn.MaxPool2d(kernel_size=3,stride=2))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _make_linear(self,in_dims,out_dims):\n",
    "        layers = []\n",
    "        layers.append(nn.Dropout(p=0.5,inplace=False))\n",
    "        layers.append(nn.Linear(in_dims,out_dims))\n",
    "        layers.append(nn.ReLU(inplace=False))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            # elif isinstance(m, nn.Linear):\n",
    "            #     nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='linear')\n",
    "            #     nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        nn.init.constant_(self.stage_2[0].bias, 1)\n",
    "        nn.init.constant_(self.stage_4[0].bias, 1)\n",
    "        nn.init.constant_(self.stage_5[0].bias, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # print(\"x\",x.shape)\n",
    "        x1 = self.stage_1(x)\n",
    "        # print(\"x1\",x1.shape)\n",
    "        x2 = self.stage_2(x1)\n",
    "        # print(\"x2\",x2.shape)\n",
    "        x3 = self.stage_3(x2)\n",
    "        # print(\"x3\",x3.shape)\n",
    "        x4 = self.stage_4(x3)\n",
    "        # print(\"x4\",x4.shape)\n",
    "        x5 = self.stage_5(x4)\n",
    "        # print(\"x5\",x5.shape)\n",
    "\n",
    "        features = x5.view(x5.size(0), -1)\n",
    "\n",
    "        features = self.lin1(features)  # MLP 1\n",
    "        # features = self.lin2(features)  # MLP 2\n",
    "        # features = self.kan1(features)  # KAN 1\n",
    "        features = self.kan2(features)['logits']  # KAN 2\n",
    "\n",
    "        # print(\"inside arch:\",features.shape)\n",
    "\n",
    "        return {\n",
    "            'fmaps': [x1,x2,x3,x4,x5],\n",
    "            'features': features\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def last_conv(self):\n",
    "        return self.stage_5[0]\n",
    "    \n",
    "    \n",
    "class AlexNetPretrained(nn.Module):\n",
    "    def __init__(self, num_classes=100, pretrained=True):\n",
    "        super(AlexNetPretrained, self).__init__()\n",
    "\n",
    "        # Load AlexNet architecture\n",
    "        self.alexnet = alexnet()\n",
    "        \n",
    "        # Load weights from the specific path if pretrained is True\n",
    "        if pretrained:\n",
    "            weights_path = \"/kaggle/input/alexnet-imagenet/alexnet-owt-7be5be79.pth\"\n",
    "            state_dict = torch.load(weights_path, weights_only=True)\n",
    "            self.alexnet.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "        # Extract AlexNet's feature extractor and classifier components\n",
    "        self.features = self.alexnet.features\n",
    "        self.out_dim = 4096\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))  # Adapts to variable input sizes\n",
    "        self.fc = nn.Linear(self.out_dim, num_classes)\n",
    "\n",
    "        # Define additional linear layers\n",
    "        self.lin1 = self._make_linear(256 * 6 * 6, 4096)  # Input size after avgpooling for AlexNet\n",
    "        # self.lin2 = KANLinear(4096, 4096)\n",
    "        self.lin2 = self._make_linear(4096, 4096)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Get feature maps\n",
    "        feature_maps = self.get_feature_maps(x)\n",
    "        \n",
    "        # Extract features using AlexNet's pre-trained layers\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        features = torch.flatten(x, 1)  # Flatten before passing to fully connected layers\n",
    "\n",
    "        # Pass through the fully connected layers\n",
    "        features = self.lin1(features)\n",
    "        features = self.lin2(features)# MLP 2\n",
    "        # features = self.lin2(features)['logits']  # KAN 2\n",
    "        \n",
    "        return {\n",
    "            'fmaps': feature_maps,\n",
    "            'features': features\n",
    "        }\n",
    "\n",
    "    def _make_linear(self, in_dims, out_dims):\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(in_dims, out_dims))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def get_feature_maps(self, x):\n",
    "        # Get intermediate feature maps after each MaxPool2d layer in AlexNet\n",
    "        feature_maps = []\n",
    "        for name, layer in self.features._modules.items():\n",
    "            x = layer(x)\n",
    "            if isinstance(layer, nn.MaxPool2d):\n",
    "                feature_maps.append(x)\n",
    "        return feature_maps\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='linear')\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    @property\n",
    "    def last_conv(self):\n",
    "        # Return the last convolutional layer in AlexNet's feature extractor\n",
    "        return self.features[-2]\n",
    "\n",
    "\n",
    "\n",
    "class DownsampleA(nn.Module):\n",
    "    def __init__(self, nIn, nOut, stride):\n",
    "        super(DownsampleA, self).__init__()\n",
    "        assert stride == 2\n",
    "        self.avg = nn.AvgPool2d(kernel_size=1, stride=stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.avg(x)\n",
    "        return torch.cat((x, x.mul(0)), 1)\n",
    "\n",
    "\n",
    "class DownsampleB(nn.Module):\n",
    "    def __init__(self, nIn, nOut, stride):\n",
    "        super(DownsampleB, self).__init__()\n",
    "        self.conv = nn.Conv2d(nIn, nOut, kernel_size=1, stride=stride, padding=0, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(nOut)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DownsampleC(nn.Module):\n",
    "    def __init__(self, nIn, nOut, stride):\n",
    "        super(DownsampleC, self).__init__()\n",
    "        assert stride != 1 or nIn != nOut\n",
    "        self.conv = nn.Conv2d(nIn, nOut, kernel_size=1, stride=stride, padding=0, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DownsampleD(nn.Module):\n",
    "    def __init__(self, nIn, nOut, stride):\n",
    "        super(DownsampleD, self).__init__()\n",
    "        assert stride == 2\n",
    "        self.conv = nn.Conv2d(nIn, nOut, kernel_size=2, stride=stride, padding=0, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(nOut)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResNetBasicblock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(ResNetBasicblock, self).__init__()\n",
    "\n",
    "        self.conv_a = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn_a = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.conv_b = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn_b = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        basicblock = self.conv_a(x)\n",
    "        basicblock = self.bn_a(basicblock)\n",
    "        basicblock = F.relu(basicblock, inplace=True)\n",
    "\n",
    "        basicblock = self.conv_b(basicblock)\n",
    "        basicblock = self.bn_b(basicblock)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        return F.relu(residual + basicblock, inplace=True)\n",
    "\n",
    "\n",
    "class CifarResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet optimized for the Cifar Dataset, as specified in\n",
    "    https://arxiv.org/abs/1512.03385.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, block, depth, channels=3):\n",
    "        super(CifarResNet, self).__init__()\n",
    "\n",
    "        # Model type specifies number of layers for CIFAR-10 and CIFAR-100 model\n",
    "        assert (depth - 2) % 6 == 0, 'depth should be one of 20, 32, 44, 56, 110'\n",
    "        layer_blocks = (depth - 2) // 6\n",
    "\n",
    "        self.conv_1_3x3 = nn.Conv2d(channels, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn_1 = nn.BatchNorm2d(16)\n",
    "\n",
    "        self.inplanes = 16\n",
    "        self.stage_1 = self._make_layer(block, 16, layer_blocks, 1)\n",
    "        self.stage_2 = self._make_layer(block, 32, layer_blocks, 2)\n",
    "        self.stage_3 = self._make_layer(block, 64, layer_blocks, 2)\n",
    "        self.avgpool = nn.AvgPool2d(8)\n",
    "        self.out_dim = 64 * block.expansion\n",
    "        self.fc = KAN([64*block.expansion, 10])\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                # m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = DownsampleA(self.inplanes, planes * block.expansion, stride)\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_1_3x3(x)  # [bs, 16, 32, 32]\n",
    "        x = F.relu(self.bn_1(x), inplace=True)\n",
    "\n",
    "        x_1 = self.stage_1(x)  # [bs, 16, 32, 32]\n",
    "        x_2 = self.stage_2(x_1)  # [bs, 32, 16, 16]\n",
    "        x_3 = self.stage_3(x_2)  # [bs, 64, 8, 8]\n",
    "\n",
    "        pooled = self.avgpool(x_3)  # [bs, 64, 1, 1]\n",
    "        features = pooled.view(pooled.size(0), -1)  # [bs, 64]\n",
    "\n",
    "        return {\n",
    "            'fmaps': [x_1, x_2, x_3],\n",
    "            'features': features\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def last_conv(self):\n",
    "        return self.stage_3[-1].conv_b\n",
    "\n",
    "\n",
    "def resnet20mnist():\n",
    "    \"\"\"Constructs a ResNet-20 model for MNIST.\"\"\"\n",
    "    model = CifarResNet(ResNetBasicblock, 20, 1)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet32mnist():\n",
    "    \"\"\"Constructs a ResNet-32 model for MNIST.\"\"\"\n",
    "    model = CifarResNet(ResNetBasicblock, 32, 1)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet20():\n",
    "    \"\"\"Constructs a ResNet-20 model for CIFAR-10.\"\"\"\n",
    "    model = CifarResNet(ResNetBasicblock, 20)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet32():\n",
    "    \"\"\"Constructs a ResNet-32 model for CIFAR-10.\"\"\"\n",
    "    # model = CifarResNet(ResNetBasicblock, 32)\n",
    "    # model = Cifar_CNN()\n",
    "    # model = Cifar_CNN_KAN()\n",
    "    # model = VGG()\n",
    "    model = VGGPretrained()\n",
    "    # model = AlexNetPretrained()\n",
    "    # model = AlexNet()\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet44():\n",
    "    \"\"\"Constructs a ResNet-44 model for CIFAR-10.\"\"\"\n",
    "    model = CifarResNet(ResNetBasicblock, 44)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet56():\n",
    "    \"\"\"Constructs a ResNet-56 model for CIFAR-10.\"\"\"\n",
    "    model = CifarResNet(ResNetBasicblock, 56)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet110():\n",
    "    \"\"\"Constructs a ResNet-110 model for CIFAR-10.\"\"\"\n",
    "    model = CifarResNet(ResNetBasicblock, 110)\n",
    "    return model\n",
    "\n",
    "# for auc\n",
    "def resnet14():\n",
    "    model = CifarResNet(ResNetBasicblock, 14)\n",
    "    return model\n",
    "\n",
    "def resnet26():\n",
    "    model = CifarResNet(ResNetBasicblock, 26)\n",
    "    return model\n",
    "\n",
    "# from torchsummary import summary\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     model = Cifar_VGG16()\n",
    "#     model.to('cuda')\n",
    "#     summary(model, input_size=(3, 32, 32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f88e012",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T15:22:33.536079Z",
     "iopub.status.busy": "2024-12-30T15:22:33.535683Z",
     "iopub.status.idle": "2024-12-30T15:22:33.541701Z",
     "shell.execute_reply": "2024-12-30T15:22:33.540908Z"
    },
    "id": "VviEPSOg6epU",
    "papermill": {
     "duration": 0.014179,
     "end_time": "2024-12-30T15:22:33.543435",
     "exception": false,
     "start_time": "2024-12-30T15:22:33.529256",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class SimpleLinear(nn.Module):\n",
    "    '''\n",
    "    Reference:\n",
    "    https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/linear.py\n",
    "    '''\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(SimpleLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, nonlinearity='linear')\n",
    "        nn.init.constant_(self.bias, 0)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return {'logits': F.linear(input, self.weight, self.bias)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafdf254",
   "metadata": {
    "id": "5pTErLiD6epV",
    "papermill": {
     "duration": 0.005619,
     "end_time": "2024-12-30T15:22:33.554734",
     "exception": false,
     "start_time": "2024-12-30T15:22:33.549115",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# BaseLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55161535",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T15:22:33.567237Z",
     "iopub.status.busy": "2024-12-30T15:22:33.566954Z",
     "iopub.status.idle": "2024-12-30T15:22:34.076379Z",
     "shell.execute_reply": "2024-12-30T15:22:34.075472Z"
    },
    "id": "nG8kagEw6epV",
    "papermill": {
     "duration": 0.518232,
     "end_time": "2024-12-30T15:22:34.078465",
     "exception": false,
     "start_time": "2024-12-30T15:22:33.560233",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import logging\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from scipy.spatial.distance import cdist\n",
    "import os\n",
    "\n",
    "EPSILON = 1e-8\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "class BaseLearner(object):\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self._cur_task = -1\n",
    "        self._known_classes = 0\n",
    "        self._total_classes = 0\n",
    "        self._network = None\n",
    "        self._old_network = None\n",
    "        self._data_memory, self._targets_memory = np.array([]), np.array([])\n",
    "        self.topk = 5\n",
    "\n",
    "        self._memory_size = args[\"memory_size\"]\n",
    "        self._memory_per_class = args.get(\"memory_per_class\", None)\n",
    "        self._fixed_memory = args.get(\"fixed_memory\", False)\n",
    "        self._device = args[\"device\"][0]\n",
    "        self._multiple_gpus = args[\"device\"]\n",
    "\n",
    "    @property\n",
    "    def exemplar_size(self):\n",
    "        assert len(self._data_memory) == len(\n",
    "            self._targets_memory\n",
    "        ), \"Exemplar size error.\"\n",
    "        return len(self._targets_memory)\n",
    "\n",
    "    @property\n",
    "    def samples_per_class(self):\n",
    "        if self._fixed_memory:\n",
    "            return self._memory_per_class\n",
    "        else:\n",
    "            assert self._total_classes != 0, \"Total classes is 0\"\n",
    "            return self._memory_size // self._total_classes\n",
    "\n",
    "    @property\n",
    "    def feature_dim(self):\n",
    "        if isinstance(self._network, nn.DataParallel):\n",
    "            return self._network.module.feature_dim\n",
    "        else:\n",
    "            return self._network.feature_dim\n",
    "\n",
    "    def build_rehearsal_memory(self, data_manager, per_class):\n",
    "        if self._fixed_memory:\n",
    "            self._construct_exemplar_unified(data_manager, per_class)\n",
    "        else:\n",
    "            self._reduce_exemplar(data_manager, per_class)\n",
    "            self._construct_exemplar(data_manager, per_class)\n",
    "\n",
    "    def save_checkpoint(self, filename):\n",
    "        self._network.cpu()\n",
    "        save_dict = {\n",
    "            \"tasks\": self._cur_task,\n",
    "            \"model_state_dict\": self._network.state_dict(),\n",
    "        }\n",
    "        torch.save(save_dict, \"{}_{}.pkl\".format(filename, self._cur_task))\n",
    "\n",
    "    def after_task(self):\n",
    "        pass\n",
    "\n",
    "    def _evaluate(self, y_pred, y_true):\n",
    "        ret = {}\n",
    "        grouped = accuracy(y_pred.T[0], y_true, self._known_classes)\n",
    "        ret[\"grouped\"] = grouped\n",
    "        ret[\"top1\"] = grouped[\"total\"]\n",
    "        ret[\"top{}\".format(self.topk)] = np.around(\n",
    "            (y_pred.T == np.tile(y_true, (self.topk, 1))).sum() * 100 / len(y_true),\n",
    "            decimals=2,\n",
    "        )\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def eval_task(self, save_conf=False):\n",
    "        y_pred, y_true = self._eval_cnn(self.test_loader)\n",
    "        cnn_accy = self._evaluate(y_pred, y_true)\n",
    "\n",
    "        if hasattr(self, \"_class_means\"):\n",
    "            y_pred, y_true = self._eval_nme(self.test_loader, self._class_means)\n",
    "            nme_accy = self._evaluate(y_pred, y_true)\n",
    "        else:\n",
    "            nme_accy = None\n",
    "\n",
    "        if save_conf:\n",
    "            _pred = y_pred.T[0]\n",
    "            _pred_path = os.path.join(self.args['logfilename'], \"pred.npy\")\n",
    "            _target_path = os.path.join(self.args['logfilename'], \"target.npy\")\n",
    "            np.save(_pred_path, _pred)\n",
    "            np.save(_target_path, y_true)\n",
    "\n",
    "            _save_dir = os.path.join(f\"./results/conf_matrix/{self.args['prefix']}\")\n",
    "            os.makedirs(_save_dir, exist_ok=True)\n",
    "            _save_path = os.path.join(_save_dir, f\"{self.args['csv_name']}.csv\")\n",
    "            with open(_save_path, \"a+\") as f:\n",
    "                f.write(f\"{self.args['time_str']},{self.args['model_name']},{_pred_path},{_target_path} \\n\")\n",
    "\n",
    "        return cnn_accy, nme_accy\n",
    "\n",
    "    def incremental_train(self):\n",
    "        pass\n",
    "\n",
    "    def _train(self):\n",
    "        pass\n",
    "\n",
    "    def _get_memory(self):\n",
    "        if len(self._data_memory) == 0:\n",
    "            return None\n",
    "        else:\n",
    "            return (self._data_memory, self._targets_memory)\n",
    "\n",
    "    def _compute_accuracy(self, model, loader):\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        for i, (_, inputs, targets) in enumerate(loader):\n",
    "            inputs = inputs.to(self._device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)[\"logits\"]\n",
    "            predicts = torch.max(outputs, dim=1)[1]\n",
    "            correct += (predicts.cpu() == targets).sum()\n",
    "            total += len(targets)\n",
    "\n",
    "        return np.around(tensor2numpy(correct) * 100 / total, decimals=2)\n",
    "\n",
    "    def _eval_cnn(self, loader):\n",
    "        self._network.eval()\n",
    "        y_pred, y_true = [], []\n",
    "        for _, (_, inputs, targets) in enumerate(loader):\n",
    "            inputs = inputs.to(self._device)\n",
    "            with torch.no_grad():\n",
    "                outputs = self._network(inputs)[\"logits\"]\n",
    "            predicts = torch.topk(\n",
    "                outputs, k=self.topk, dim=1, largest=True, sorted=True\n",
    "            )[\n",
    "                1\n",
    "            ]  # [bs, topk]\n",
    "            y_pred.append(predicts.cpu().numpy())\n",
    "            y_true.append(targets.cpu().numpy())\n",
    "\n",
    "        return np.concatenate(y_pred), np.concatenate(y_true)  # [N, topk]\n",
    "\n",
    "    def _eval_nme(self, loader, class_means):\n",
    "        self._network.eval()\n",
    "        vectors, y_true = self._extract_vectors(loader)\n",
    "        vectors = (vectors.T / (np.linalg.norm(vectors.T, axis=0) + EPSILON)).T\n",
    "\n",
    "        dists = cdist(class_means, vectors, \"sqeuclidean\")  # [nb_classes, N]\n",
    "        scores = dists.T  # [N, nb_classes], choose the one with the smallest distance\n",
    "\n",
    "        return np.argsort(scores, axis=1)[:, : self.topk], y_true  # [N, topk]\n",
    "\n",
    "    def _extract_vectors(self, loader):\n",
    "        self._network.eval()\n",
    "        vectors, targets = [], []\n",
    "        for _, _inputs, _targets in loader:  # no issues here, data is being retrieved\n",
    "            _targets = _targets.numpy()\n",
    "            if isinstance(self._network, nn.DataParallel):\n",
    "                _vectors = tensor2numpy(\n",
    "                    self._network.module.extract_vector(_inputs.to(self._device))\n",
    "                )\n",
    "            else:\n",
    "                _vectors = tensor2numpy(\n",
    "                    self._network.extract_vector(_inputs.to(self._device))\n",
    "                )\n",
    "\n",
    "            vectors.append(_vectors)\n",
    "            targets.append(_targets)\n",
    "\n",
    "        return np.concatenate(vectors), np.concatenate(targets)\n",
    "\n",
    "    def _reduce_exemplar(self, data_manager, m):\n",
    "        logging.info(\"Reducing exemplars...({} per classes)\".format(m))\n",
    "        dummy_data, dummy_targets = copy.deepcopy(self._data_memory), copy.deepcopy(\n",
    "            self._targets_memory\n",
    "        )\n",
    "        self._class_means = np.zeros((self._total_classes, self.feature_dim))\n",
    "        self._data_memory, self._targets_memory = np.array([]), np.array([])\n",
    "\n",
    "        for class_idx in range(self._known_classes):\n",
    "            mask = np.where(dummy_targets == class_idx)[0]\n",
    "            dd, dt = dummy_data[mask][:m], dummy_targets[mask][:m]\n",
    "            self._data_memory = (\n",
    "                np.concatenate((self._data_memory, dd))\n",
    "                if len(self._data_memory) != 0\n",
    "                else dd\n",
    "            )\n",
    "            self._targets_memory = (\n",
    "                np.concatenate((self._targets_memory, dt))\n",
    "                if len(self._targets_memory) != 0\n",
    "                else dt\n",
    "            )\n",
    "\n",
    "            # Exemplar mean\n",
    "            idx_dataset = data_manager.get_dataset(\n",
    "                [], source=\"train\", mode=\"test\", appendent=(dd, dt)\n",
    "            )\n",
    "            idx_loader = DataLoader(\n",
    "                idx_dataset, batch_size=batch_size, shuffle=False, num_workers=4\n",
    "            )\n",
    "            vectors, _ = self._extract_vectors(idx_loader)\n",
    "            vectors = (vectors.T / (np.linalg.norm(vectors.T, axis=0) + EPSILON)).T\n",
    "            mean = np.mean(vectors, axis=0)\n",
    "            mean = mean / np.linalg.norm(mean)\n",
    "\n",
    "            self._class_means[class_idx, :] = mean\n",
    "\n",
    "    def _construct_exemplar(self, data_manager, m):\n",
    "        logging.info(\"Constructing exemplars...({} per classes)\".format(m))\n",
    "        for class_idx in range(self._known_classes, self._total_classes):\n",
    "            data, targets, idx_dataset = data_manager.get_dataset(\n",
    "                np.arange(class_idx, class_idx + 1),\n",
    "                source=\"train\",\n",
    "                mode=\"test\",\n",
    "                ret_data=True,\n",
    "            )\n",
    "            idx_loader = DataLoader(\n",
    "                idx_dataset, batch_size=batch_size, shuffle=False, num_workers=4\n",
    "            )\n",
    "            vectors, _ = self._extract_vectors(idx_loader)\n",
    "            vectors = (vectors.T / (np.linalg.norm(vectors.T, axis=0) + EPSILON)).T\n",
    "            class_mean = np.mean(vectors, axis=0)\n",
    "\n",
    "            # Select\n",
    "            selected_exemplars = []\n",
    "            exemplar_vectors = []  # [n, feature_dim]\n",
    "            for k in range(1, m + 1):\n",
    "                S = np.sum(exemplar_vectors, axis=0)  # Sum of selected exemplars vectors\n",
    "                mu_p = (vectors + S) / k  # Sum to all vectors\n",
    "\n",
    "                i = np.argmin(np.sqrt(np.sum((class_mean - mu_p) ** 2, axis=1)))\n",
    "                selected_exemplars.append(np.array(data[i]))  # Avoid passing by inference\n",
    "                exemplar_vectors.append(np.array(vectors[i]))  # Avoid passing by inference\n",
    "\n",
    "                vectors = np.delete(vectors, i, axis=0)  # Remove to avoid duplicative selection\n",
    "                data = np.delete(data, i, axis=0)  # Remove to avoid duplicative selection\n",
    "\n",
    "            # uniques = np.unique(selected_exemplars, axis=0)\n",
    "            # print('Unique elements: {}'.format(len(uniques)))\n",
    "            selected_exemplars = np.array(selected_exemplars)\n",
    "            exemplar_targets = np.full(m, class_idx)\n",
    "            self._data_memory = (\n",
    "                np.concatenate((self._data_memory, selected_exemplars))\n",
    "                if len(self._data_memory) != 0\n",
    "                else selected_exemplars\n",
    "            )\n",
    "            self._targets_memory = (\n",
    "                np.concatenate((self._targets_memory, exemplar_targets))\n",
    "                if len(self._targets_memory) != 0\n",
    "                else exemplar_targets\n",
    "            )\n",
    "\n",
    "            # Exemplar mean\n",
    "            idx_dataset = data_manager.get_dataset(\n",
    "                [],\n",
    "                source=\"train\",\n",
    "                mode=\"test\",\n",
    "                appendent=(selected_exemplars, exemplar_targets),\n",
    "            )\n",
    "            idx_loader = DataLoader(\n",
    "                idx_dataset, batch_size=batch_size, shuffle=False, num_workers=4\n",
    "            )\n",
    "            vectors, _ = self._extract_vectors(idx_loader)\n",
    "            vectors = (vectors.T / (np.linalg.norm(vectors.T, axis=0) + EPSILON)).T\n",
    "            mean = np.mean(vectors, axis=0)\n",
    "            mean = mean / np.linalg.norm(mean)\n",
    "\n",
    "            self._class_means[class_idx, :] = mean\n",
    "\n",
    "    def _construct_exemplar_unified(self, data_manager, m):\n",
    "        logging.info(\n",
    "            \"Constructing exemplars for new classes...({} per classes)\".format(m)\n",
    "        )\n",
    "        _class_means = np.zeros((self._total_classes, self.feature_dim))\n",
    "\n",
    "        # Calculate the means of old classes with newly trained network\n",
    "        for class_idx in range(self._known_classes):\n",
    "            mask = np.where(self._targets_memory == class_idx)[0]\n",
    "            class_data, class_targets = (\n",
    "                self._data_memory[mask],\n",
    "                self._targets_memory[mask],\n",
    "            )\n",
    "\n",
    "            class_dset = data_manager.get_dataset(\n",
    "                [], source=\"train\", mode=\"test\", appendent=(class_data, class_targets)\n",
    "            )\n",
    "            class_loader = DataLoader(\n",
    "                class_dset, batch_size=batch_size, shuffle=False, num_workers=4\n",
    "            )\n",
    "            vectors, _ = self._extract_vectors(class_loader)\n",
    "            vectors = (vectors.T / (np.linalg.norm(vectors.T, axis=0) + EPSILON)).T\n",
    "            mean = np.mean(vectors, axis=0)\n",
    "            mean = mean / np.linalg.norm(mean)\n",
    "\n",
    "            _class_means[class_idx, :] = mean\n",
    "\n",
    "        # Construct exemplars for new classes and calculate the means\n",
    "        for class_idx in range(self._known_classes, self._total_classes):\n",
    "            data, targets, class_dset = data_manager.get_dataset(\n",
    "                np.arange(class_idx, class_idx + 1),\n",
    "                source=\"train\",\n",
    "                mode=\"test\",\n",
    "                ret_data=True,\n",
    "            )\n",
    "            class_loader = DataLoader(\n",
    "                class_dset, batch_size=batch_size, shuffle=False, num_workers=4\n",
    "            )\n",
    "\n",
    "            vectors, _ = self._extract_vectors(class_loader)\n",
    "            vectors = (vectors.T / (np.linalg.norm(vectors.T, axis=0) + EPSILON)).T\n",
    "            class_mean = np.mean(vectors, axis=0)\n",
    "\n",
    "            # Select\n",
    "            selected_exemplars = []\n",
    "            exemplar_vectors = []\n",
    "            for k in range(1, m + 1):\n",
    "                S = np.sum(\n",
    "                    exemplar_vectors, axis=0\n",
    "                )  # [feature_dim] sum of selected exemplars vectors\n",
    "                mu_p = (vectors + S) / k  # [n, feature_dim] sum to all vectors\n",
    "                i = np.argmin(np.sqrt(np.sum((class_mean - mu_p) ** 2, axis=1)))\n",
    "\n",
    "                selected_exemplars.append(\n",
    "                    np.array(data[i])\n",
    "                )  # New object to avoid passing by inference\n",
    "                exemplar_vectors.append(\n",
    "                    np.array(vectors[i])\n",
    "                )  # New object to avoid passing by inference\n",
    "\n",
    "                vectors = np.delete(\n",
    "                    vectors, i, axis=0\n",
    "                )  # Remove it to avoid duplicative selection\n",
    "                data = np.delete(\n",
    "                    data, i, axis=0\n",
    "                )  # Remove it to avoid duplicative selection\n",
    "\n",
    "            selected_exemplars = np.array(selected_exemplars)\n",
    "            exemplar_targets = np.full(m, class_idx)\n",
    "            self._data_memory = (\n",
    "                np.concatenate((self._data_memory, selected_exemplars))\n",
    "                if len(self._data_memory) != 0\n",
    "                else selected_exemplars\n",
    "            )\n",
    "            self._targets_memory = (\n",
    "                np.concatenate((self._targets_memory, exemplar_targets))\n",
    "                if len(self._targets_memory) != 0\n",
    "                else exemplar_targets\n",
    "            )\n",
    "\n",
    "            # Exemplar mean\n",
    "            exemplar_dset = data_manager.get_dataset(\n",
    "                [],\n",
    "                source=\"train\",\n",
    "                mode=\"test\",\n",
    "                appendent=(selected_exemplars, exemplar_targets),\n",
    "            )\n",
    "            exemplar_loader = DataLoader(\n",
    "                exemplar_dset, batch_size=batch_size, shuffle=False, num_workers=4\n",
    "            )\n",
    "            vectors, _ = self._extract_vectors(exemplar_loader)\n",
    "            vectors = (vectors.T / (np.linalg.norm(vectors.T, axis=0) + EPSILON)).T\n",
    "            mean = np.mean(vectors, axis=0)\n",
    "            mean = mean / np.linalg.norm(mean)\n",
    "\n",
    "            _class_means[class_idx, :] = mean\n",
    "\n",
    "        self._class_means = _class_means\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bea2a2f",
   "metadata": {
    "id": "9BKamODY6epW",
    "papermill": {
     "duration": 0.006083,
     "end_time": "2024-12-30T15:22:34.090687",
     "exception": false,
     "start_time": "2024-12-30T15:22:34.084604",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Incremental Net + hyperparams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "beb1ed81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T15:22:34.109235Z",
     "iopub.status.busy": "2024-12-30T15:22:34.108846Z",
     "iopub.status.idle": "2024-12-30T15:22:34.141094Z",
     "shell.execute_reply": "2024-12-30T15:22:34.140195Z"
    },
    "id": "oSH-Jmbs6epW",
    "papermill": {
     "duration": 0.043821,
     "end_time": "2024-12-30T15:22:34.143284",
     "exception": false,
     "start_time": "2024-12-30T15:22:34.099463",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import logging\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "def get_convnet(args, pretrained=False):\n",
    "    name = args[\"convnet_type\"].lower()\n",
    "    if name == \"resnet32\":\n",
    "        return resnet32()\n",
    "    elif name == \"resnet18\":\n",
    "        return resnet18(pretrained=pretrained,args=args)\n",
    "    elif name == \"resnet34\":\n",
    "        return resnet34(pretrained=pretrained,args=args)\n",
    "    elif name == \"resnet50\":\n",
    "        return resnet50(pretrained=pretrained,args=args)\n",
    "    elif name == \"cosine_resnet18\":\n",
    "        return cosine_resnet18(pretrained=pretrained,args=args)\n",
    "    elif name == \"cosine_resnet32\":\n",
    "        return cosine_resnet32()\n",
    "    elif name == \"cosine_resnet34\":\n",
    "        return cosine_resnet34(pretrained=pretrained,args=args)\n",
    "    elif name == \"cosine_resnet50\":\n",
    "        return cosine_resnet50(pretrained=pretrained,args=args)\n",
    "    elif name == \"resnet18_rep\":\n",
    "        return resnet18_rep(pretrained=pretrained,args=args)\n",
    "    elif name == \"resnet18_cbam\":\n",
    "        return resnet18_cbam(pretrained=pretrained,args=args)\n",
    "    elif name == \"resnet34_cbam\":\n",
    "        return resnet34_cbam(pretrained=pretrained,args=args)\n",
    "    elif name == \"resnet50_cbam\":\n",
    "        return resnet50_cbam(pretrained=pretrained,args=args)\n",
    "\n",
    "    # MEMO benchmark backbone\n",
    "    elif name == 'memo_resnet18':\n",
    "        _basenet, _adaptive_net = get_memo_resnet18()\n",
    "        return _basenet, _adaptive_net\n",
    "    elif name == 'memo_resnet32':\n",
    "        _basenet, _adaptive_net = get_memo_resnet32()\n",
    "        return _basenet, _adaptive_net\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(\"Unknown type {}\".format(name))\n",
    "\n",
    "\n",
    "class BaseNet(nn.Module):\n",
    "    def __init__(self, args, pretrained):\n",
    "        super(BaseNet, self).__init__()\n",
    "\n",
    "        self.convnet = get_convnet(args, pretrained)\n",
    "        self.fc = None\n",
    "\n",
    "    @property\n",
    "    def feature_dim(self):\n",
    "        return self.convnet.out_dim\n",
    "\n",
    "    def extract_vector(self, x):\n",
    "        return self.convnet(x)[\"features\"]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convnet(x)\n",
    "        out = self.fc(x[\"features\"])\n",
    "        \"\"\"\n",
    "        {\n",
    "            'fmaps': [x_1, x_2, ..., x_n],\n",
    "            'features': features\n",
    "            'logits': logits\n",
    "        }\n",
    "        \"\"\"\n",
    "        out.update(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def update_fc(self, nb_classes):\n",
    "        pass\n",
    "\n",
    "    def generate_fc(self, in_dim, out_dim):\n",
    "        pass\n",
    "\n",
    "    def copy(self):\n",
    "        return copy.deepcopy(self)\n",
    "\n",
    "    def freeze(self):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.eval()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def load_checkpoint(self, args):\n",
    "        if args[\"init_cls\"] == 50:\n",
    "            pkl_name = \"{}_{}_{}_B{}_Inc{}\".format(\n",
    "                args[\"dataset\"],\n",
    "                args[\"seed\"],\n",
    "                args[\"convnet_type\"],\n",
    "                0,\n",
    "                args[\"init_cls\"],\n",
    "            )\n",
    "            checkpoint_name = f\"checkpoints/finetune_{pkl_name}_0.pkl\"\n",
    "        else:\n",
    "            checkpoint_name = f\"checkpoints/finetune_{args['csv_name']}_0.pkl\"\n",
    "        model_infos = torch.load(checkpoint_name)\n",
    "        self.convnet.load_state_dict(model_infos['convnet'])\n",
    "        self.fc.load_state_dict(model_infos['fc'])\n",
    "        test_acc = model_infos['test_acc']\n",
    "        return test_acc\n",
    "\n",
    "class IncrementalNet(BaseNet):\n",
    "    def __init__(self, args, pretrained, gradcam=False):\n",
    "        super().__init__(args, pretrained)\n",
    "        self.gradcam = gradcam\n",
    "        if hasattr(self, \"gradcam\") and self.gradcam:\n",
    "            self._gradcam_hooks = [None, None]\n",
    "            self.set_gradcam_hook()\n",
    "\n",
    "    def update_fc(self, nb_classes):\n",
    "        # fc = self.generate_fc(self.feature_dim, nb_classes)\n",
    "        fc = self.generate_fc(4096, nb_classes)\n",
    "        if self.fc is not None:\n",
    "            print(\"self.fc: \\n\\n\",self.fc,\"\\n\\n\")\n",
    "            nb_output = self.fc.out_features\n",
    "\n",
    "            # MLP\n",
    "            weight = copy.deepcopy(self.fc.weight.data)\n",
    "            bias = copy.deepcopy(self.fc.bias.data)\n",
    "            fc.weight.data[:nb_output] = weight\n",
    "            fc.bias.data[:nb_output] = bias\n",
    "\n",
    "            # KAN experiment\n",
    "            # spline_weight = copy.deepcopy(self.fc.spline_weight.data)\n",
    "            # base_weight = copy.deepcopy(self.fc.base_weight.data)\n",
    "            # fc.spline_weight.data[:nb_output] = spline_weight\n",
    "            # fc.base_weight.data[:nb_output] = base_weight\n",
    "\n",
    "        del self.fc\n",
    "        self.fc = fc\n",
    "\n",
    "    def weight_align(self, increment):\n",
    "        weights = self.fc.weight.data\n",
    "        newnorm = torch.norm(weights[-increment:, :], p=2, dim=1)\n",
    "        oldnorm = torch.norm(weights[:-increment, :], p=2, dim=1)\n",
    "        meannew = torch.mean(newnorm)\n",
    "        meanold = torch.mean(oldnorm)\n",
    "        gamma = meanold / meannew\n",
    "        print(\"alignweights,gamma=\", gamma)\n",
    "        self.fc.weight.data[-increment:, :] *= gamma\n",
    "\n",
    "    def generate_fc(self, in_dim, out_dim):\n",
    "        fc = SimpleLinear(in_dim, out_dim)\n",
    "        # fc = KANLinear(in_dim,out_dim)\n",
    "        return fc\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convnet(x)\n",
    "        out = self.fc(x[\"features\"])\n",
    "        out.update(x)\n",
    "        if hasattr(self, \"gradcam\") and self.gradcam:\n",
    "            out[\"gradcam_gradients\"] = self._gradcam_gradients\n",
    "            out[\"gradcam_activations\"] = self._gradcam_activations\n",
    "\n",
    "        return out\n",
    "\n",
    "    def unset_gradcam_hook(self):\n",
    "        self._gradcam_hooks[0].remove()\n",
    "        self._gradcam_hooks[1].remove()\n",
    "        self._gradcam_hooks[0] = None\n",
    "        self._gradcam_hooks[1] = None\n",
    "        self._gradcam_gradients, self._gradcam_activations = [None], [None]\n",
    "\n",
    "    def set_gradcam_hook(self):\n",
    "        self._gradcam_gradients, self._gradcam_activations = [None], [None]\n",
    "\n",
    "        def backward_hook(module, grad_input, grad_output):\n",
    "            self._gradcam_gradients[0] = grad_output[0]\n",
    "            return None\n",
    "\n",
    "        def forward_hook(module, input, output):\n",
    "            self._gradcam_activations[0] = output\n",
    "            return None\n",
    "\n",
    "        self._gradcam_hooks[0] = self.convnet.last_conv.register_backward_hook(\n",
    "            backward_hook\n",
    "        )\n",
    "        self._gradcam_hooks[1] = self.convnet.last_conv.register_forward_hook(\n",
    "            forward_hook\n",
    "        )\n",
    "\n",
    "class CosineIncrementalNet(BaseNet):\n",
    "    def __init__(self, args, pretrained, nb_proxy=1):\n",
    "        super().__init__(args, pretrained)\n",
    "        self.nb_proxy = nb_proxy\n",
    "\n",
    "    def update_fc(self, nb_classes, task_num):\n",
    "        fc = self.generate_fc(self.feature_dim, nb_classes)\n",
    "        if self.fc is not None:\n",
    "            if task_num == 1:\n",
    "                fc.fc1.weight.data = self.fc.weight.data\n",
    "                fc.sigma.data = self.fc.sigma.data\n",
    "            else:\n",
    "                prev_out_features1 = self.fc.fc1.out_features\n",
    "                fc.fc1.weight.data[:prev_out_features1] = self.fc.fc1.weight.data\n",
    "                fc.fc1.weight.data[prev_out_features1:] = self.fc.fc2.weight.data\n",
    "                fc.sigma.data = self.fc.sigma.data\n",
    "\n",
    "        del self.fc\n",
    "        self.fc = fc\n",
    "\n",
    "    def generate_fc(self, in_dim, out_dim):\n",
    "        if self.fc is None:\n",
    "            fc = CosineLinear(in_dim, out_dim, self.nb_proxy, to_reduce=True)\n",
    "        else:\n",
    "            prev_out_features = self.fc.out_features // self.nb_proxy\n",
    "            # prev_out_features = self.fc.out_features\n",
    "            fc = SplitCosineLinear(\n",
    "                in_dim, prev_out_features, out_dim - prev_out_features, self.nb_proxy\n",
    "            )\n",
    "\n",
    "        return fc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64907a41",
   "metadata": {
    "papermill": {
     "duration": 0.009817,
     "end_time": "2024-12-30T15:22:34.166508",
     "exception": false,
     "start_time": "2024-12-30T15:22:34.156691",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## LWF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1267f749",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-30T15:22:34.189370Z",
     "iopub.status.busy": "2024-12-30T15:22:34.188923Z",
     "iopub.status.idle": "2024-12-30T15:22:34.220544Z",
     "shell.execute_reply": "2024-12-30T15:22:34.219947Z"
    },
    "id": "KsYKCfRg6epW",
    "papermill": {
     "duration": 0.046147,
     "end_time": "2024-12-30T15:22:34.222224",
     "exception": false,
     "start_time": "2024-12-30T15:22:34.176077",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.serialization import load\n",
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "init_epoch = 150 \n",
    "init_lr = 0.001 \n",
    "init_milestones = [60, 120, 160]\n",
    "init_lr_decay = 0.1\n",
    "init_weight_decay = 0.0005\n",
    "\n",
    "epochs = 200\n",
    "lrate = 0.001\n",
    "milestones = [60, 120, 160, 180]\n",
    "lrate_decay = 0.01\n",
    "batch_size = 32\n",
    "weight_decay = 0.0002\n",
    "num_workers = 2\n",
    "T = 2\n",
    "lamda = 3\n",
    "\n",
    "# init_epoch = 100 \n",
    "# init_lr = 0.001 \n",
    "# init_milestones = [40,70,90] #[60, 120, 160]\n",
    "# init_lr_decay = 0.1\n",
    "# init_weight_decay = 0.0005\n",
    "\n",
    "# epochs = 150\n",
    "# lrate = 0.001\n",
    "# milestones = [40,70,90,120] #[60, 120, 180, 220]\n",
    "# lrate_decay = 0.01\n",
    "# batch_size = 32\n",
    "# weight_decay = 0.0002\n",
    "# num_workers = 2\n",
    "# T = 2\n",
    "# lamda = 3\n",
    "\n",
    "\n",
    "class LwF(BaseLearner):\n",
    "    def __init__(self, args):\n",
    "        super().__init__(args)\n",
    "        logging.info(f\"init_epoch = {init_epoch} \\ninit_lr = {init_lr} \\ninit_milestones = {init_milestones}\\ninit_lr_decay = {init_lr_decay}\\ninit_weight_decay = {init_weight_decay}\\n\\nepochs = {epochs}\\nlrate = {lrate}\\nmilestones = {milestones}\\nlrate_decay = {lrate_decay}\\nbatch_size = {batch_size}\\nweight_decay = {weight_decay}\\nnum_workers = {num_workers}\\nT = {T}\\nlamda = {lamda}\")\n",
    "        self._network = IncrementalNet(args, False)\n",
    "\n",
    "    def after_task(self):\n",
    "        self._old_network = self._network.copy().freeze()\n",
    "        self._known_classes = self._total_classes\n",
    "\n",
    "    def incremental_train(self, data_manager):\n",
    "        self._cur_task += 1\n",
    "        self._total_classes = self._known_classes + data_manager.get_task_size(\n",
    "            self._cur_task\n",
    "        )\n",
    "        self._network.update_fc(self._total_classes)\n",
    "        logging.info(\n",
    "            \"Learning on {}-{}\".format(self._known_classes, self._total_classes)\n",
    "        )\n",
    "\n",
    "        train_dataset = data_manager.get_dataset(\n",
    "            np.arange(self._known_classes, self._total_classes),\n",
    "            source=\"train\",\n",
    "            mode=\"train\",\n",
    "        )\n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers\n",
    "        )\n",
    "        test_dataset = data_manager.get_dataset(\n",
    "            np.arange(0, self._total_classes), source=\"test\", mode=\"test\"\n",
    "        )\n",
    "        self.test_loader = DataLoader(\n",
    "            test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers\n",
    "        )\n",
    "\n",
    "        if len(self._multiple_gpus) > 1:\n",
    "            self._network = nn.DataParallel(self._network, self._multiple_gpus)\n",
    "        self._train(self.train_loader, self.test_loader)\n",
    "        if len(self._multiple_gpus) > 1:\n",
    "            self._network = self._network.module\n",
    "\n",
    "    def _train(self, train_loader, test_loader):\n",
    "        self._network.to(self._device)\n",
    "        if self._old_network is not None:\n",
    "            self._old_network.to(self._device)\n",
    "\n",
    "        if self._cur_task == 0:\n",
    "            optimizer = optim.SGD(\n",
    "                self._network.parameters(),\n",
    "                momentum=0.9,\n",
    "                lr=init_lr,\n",
    "                weight_decay=init_weight_decay,\n",
    "            )\n",
    "            scheduler = optim.lr_scheduler.MultiStepLR(\n",
    "                optimizer=optimizer, milestones=init_milestones, gamma=init_lr_decay\n",
    "            )\n",
    "            self._init_train(train_loader, test_loader, optimizer, scheduler)\n",
    "        else:\n",
    "            optimizer = optim.SGD(\n",
    "                self._network.parameters(),\n",
    "                lr=lrate,\n",
    "                momentum=0.9,\n",
    "                weight_decay=weight_decay,\n",
    "            )\n",
    "            scheduler = optim.lr_scheduler.MultiStepLR(\n",
    "                optimizer=optimizer, milestones=milestones, gamma=lrate_decay\n",
    "            )\n",
    "            self._update_representation(train_loader, test_loader, optimizer, scheduler)\n",
    "\n",
    "    def _init_train(self, train_loader, test_loader, optimizer, scheduler):\n",
    "        prog_bar = tqdm(range(init_epoch))\n",
    "        for _, epoch in enumerate(prog_bar):\n",
    "\n",
    "            if epoch == 150:  # Freeze layers after 100 epochs\n",
    "                self._freeze_except_last_two()\n",
    "\n",
    "                # Reinitialize optimizer to exclude frozen parameters\n",
    "                optimizer = optim.SGD(\n",
    "                    filter(lambda p: p.requires_grad, self._network.parameters()),\n",
    "                    lr=lrate,\n",
    "                    momentum=0.9,\n",
    "                    weight_decay=weight_decay,\n",
    "                )\n",
    "                scheduler = optim.lr_scheduler.MultiStepLR(\n",
    "                    optimizer=optimizer, milestones=milestones, gamma=lrate_decay\n",
    "                )\n",
    "            \n",
    "            self._network.train()\n",
    "            losses = 0.0\n",
    "            correct, total = 0, 0\n",
    "            for i, (_, inputs, targets) in enumerate(train_loader):\n",
    "                # print(\"tes\",inputs.shape,targets.shape)\n",
    "                inputs, targets = inputs.to(self._device), targets.to(self._device)\n",
    "                logits = self._network(inputs)[\"logits\"]\n",
    "                # print(\"debug\",logits.shape,targets.shape)\n",
    "                loss = F.cross_entropy(logits, targets)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                losses += loss.item()\n",
    "\n",
    "                _, preds = torch.max(logits, dim=1)\n",
    "                correct += preds.eq(targets.expand_as(preds)).cpu().sum()\n",
    "                total += len(targets)\n",
    "\n",
    "            scheduler.step()\n",
    "            train_acc = np.around(tensor2numpy(correct) * 100 / total, decimals=2)\n",
    "\n",
    "            if epoch % 5 == 0:\n",
    "                test_acc = self._compute_accuracy(self._network, test_loader)\n",
    "                info = \"Task {}, Epoch {}/{} => Loss {:.3f}, Train_accy {:.2f}, Test_accy {:.2f}\".format(\n",
    "                    self._cur_task,\n",
    "                    epoch + 1,\n",
    "                    init_epoch,\n",
    "                    losses / len(train_loader),\n",
    "                    train_acc,\n",
    "                    test_acc,\n",
    "                )\n",
    "            else:\n",
    "                info = \"Task {}, Epoch {}/{} => Loss {:.3f}, Train_accy {:.2f}\".format(\n",
    "                    self._cur_task,\n",
    "                    epoch + 1,\n",
    "                    init_epoch,\n",
    "                    losses / len(train_loader),\n",
    "                    train_acc,\n",
    "                )\n",
    "            prog_bar.set_description(info)\n",
    "\n",
    "        logging.info(info)\n",
    "        \n",
    "        # Unfreeze layers for the next task\n",
    "        self._unfreeze_all()\n",
    "\n",
    "    def _update_representation(self, train_loader, test_loader, optimizer, scheduler):\n",
    "\n",
    "        prog_bar = tqdm(range(epochs))\n",
    "        for _, epoch in enumerate(prog_bar):\n",
    "\n",
    "            if epoch == 150:  # Freeze layers after 100 epochs\n",
    "                self._freeze_except_last_two()\n",
    "\n",
    "                # Reinitialize optimizer to exclude frozen parameters\n",
    "                optimizer = optim.SGD(\n",
    "                    filter(lambda p: p.requires_grad, self._network.parameters()),\n",
    "                    lr=lrate,\n",
    "                    momentum=0.9,\n",
    "                    weight_decay=weight_decay,\n",
    "                )\n",
    "                scheduler = optim.lr_scheduler.MultiStepLR(\n",
    "                    optimizer=optimizer, milestones=milestones, gamma=lrate_decay\n",
    "                )\n",
    "            \n",
    "            self._network.train()\n",
    "            losses = 0.0\n",
    "            correct, total = 0, 0\n",
    "            for i, (_, inputs, targets) in enumerate(train_loader):\n",
    "                inputs, targets = inputs.to(self._device), targets.to(self._device)\n",
    "                logits = self._network(inputs)[\"logits\"]\n",
    "\n",
    "                fake_targets = targets - self._known_classes\n",
    "                loss_clf = F.cross_entropy(\n",
    "                    logits[:, self._known_classes :], fake_targets\n",
    "                )\n",
    "                loss_kd = _KD_loss(\n",
    "                    logits[:, : self._known_classes],\n",
    "                    self._old_network(inputs)[\"logits\"],\n",
    "                    T,\n",
    "                )\n",
    "\n",
    "                loss = lamda * loss_kd + loss_clf\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                losses += loss.item()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    _, preds = torch.max(logits, dim=1)\n",
    "                    correct += preds.eq(targets.expand_as(preds)).cpu().sum()\n",
    "                    total += len(targets)\n",
    "\n",
    "            scheduler.step()\n",
    "            train_acc = np.around(tensor2numpy(correct) * 100 / total, decimals=2)\n",
    "            if epoch % 5 == 0:\n",
    "                test_acc = self._compute_accuracy(self._network, test_loader)\n",
    "                info = \"Task {}, Epoch {}/{} => Loss {:.3f}, Train_accy {:.2f}, Test_accy {:.2f}\".format(\n",
    "                    self._cur_task,\n",
    "                    epoch + 1,\n",
    "                    epochs,\n",
    "                    losses / len(train_loader),\n",
    "                    train_acc,\n",
    "                    test_acc,\n",
    "                )\n",
    "            else:\n",
    "                info = \"Task {}, Epoch {}/{} => Loss {:.3f}, Train_accy {:.2f}\".format(\n",
    "                    self._cur_task,\n",
    "                    epoch + 1,\n",
    "                    epochs,\n",
    "                    losses / len(train_loader),\n",
    "                    train_acc,\n",
    "                )\n",
    "            prog_bar.set_description(info)\n",
    "        logging.info(info)\n",
    "        \n",
    "        # Unfreeze layers for the next task\n",
    "        self._unfreeze_all()\n",
    "\n",
    "\n",
    "    def _freeze_except_last_two(self):\n",
    "        # Freeze all layers except the last two\n",
    "        layer_count = len(list(self._network.parameters()))\n",
    "        for i, param in enumerate(self._network.parameters()):\n",
    "            if i < layer_count - 2:  # Freeze all layers except the last two\n",
    "                param.requires_grad = False\n",
    "            else:\n",
    "                param.requires_grad = True\n",
    "        logging.info(\"Frozen all layers except the last two.\")\n",
    "        \n",
    "    def _unfreeze_all(self):\n",
    "        # Unfreeze all layers for the next task\n",
    "        for param in self._network.parameters():\n",
    "            param.requires_grad = True\n",
    "        logging.info(\"Unfrozen all layers for the next task.\")\n",
    "\n",
    "        optimizer = optim.SGD(\n",
    "            self._network.parameters(),\n",
    "            lr=lrate,\n",
    "            momentum=0.9,\n",
    "            weight_decay=weight_decay,\n",
    "        )\n",
    "        scheduler = optim.lr_scheduler.MultiStepLR(\n",
    "            optimizer=optimizer, milestones=milestones, gamma=lrate_decay\n",
    "        )\n",
    "\n",
    "\n",
    "def _KD_loss(pred, soft, T):\n",
    "    pred = torch.log_softmax(pred / T, dim=1)\n",
    "    soft = torch.softmax(soft / T, dim=1)\n",
    "    return -1 * torch.mul(soft, pred).sum() / pred.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f198b3",
   "metadata": {
    "papermill": {
     "duration": 0.005896,
     "end_time": "2024-12-30T15:22:34.234157",
     "exception": false,
     "start_time": "2024-12-30T15:22:34.228261",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## iCarl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a2081ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T15:22:34.250199Z",
     "iopub.status.busy": "2024-12-30T15:22:34.249906Z",
     "iopub.status.idle": "2024-12-30T15:22:34.278310Z",
     "shell.execute_reply": "2024-12-30T15:22:34.277536Z"
    },
    "papermill": {
     "duration": 0.037773,
     "end_time": "2024-12-30T15:22:34.279880",
     "exception": false,
     "start_time": "2024-12-30T15:22:34.242107",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "EPSILON = 1e-8\n",
    "\n",
    "# CIFAR-100\n",
    "# init_epoch = 70\n",
    "# init_lr = 2\n",
    "# init_milestones = [49,63]\n",
    "# init_lr_decay = 0.2  # divide by 5\n",
    "# init_weight_decay = 0.00001\n",
    "\n",
    "\n",
    "# epochs = 70\n",
    "# lrate = 2\n",
    "# milestones = [49,63]\n",
    "# lrate_decay = 0.2\n",
    "# batch_size = 128\n",
    "# weight_decay = 0.00001\n",
    "# num_workers = 2\n",
    "# T = 2\n",
    "\n",
    "# init_epoch = 100 \n",
    "# init_lr = 0.001 \n",
    "# init_milestones = [60, 120, 160]\n",
    "# init_lr_decay = 0.001\n",
    "# init_weight_decay = 0.0005\n",
    "\n",
    "# epochs = 150\n",
    "# lrate = 0.001\n",
    "# milestones = [60, 120, 180, 220]\n",
    "# lrate_decay = 0.001\n",
    "# batch_size = 32\n",
    "# weight_decay = 0.0002\n",
    "# num_workers = 2\n",
    "# T = 2\n",
    "# lamda = 3\n",
    "\n",
    "\n",
    "class iCaRL(BaseLearner):\n",
    "    def __init__(self, args):\n",
    "        super().__init__(args)\n",
    "        self._network = IncrementalNet(args, False)\n",
    "        logging.info(f\"init_epoch = {init_epoch} \\ninit_lr = {init_lr} \\ninit_milestones = {init_milestones}\\ninit_lr_decay = {init_lr_decay}\\ninit_weight_decay = {init_weight_decay}\\n\\nepochs = {epochs}\\nlrate = {lrate}\\nmilestones = {milestones}\\nlrate_decay = {lrate_decay}\\nbatch_size = {batch_size}\\nweight_decay = {weight_decay}\\nnum_workers = {num_workers}\\nT = {T}\")\n",
    "\n",
    "\n",
    "    def after_task(self):\n",
    "        self._old_network = self._network.copy().freeze()\n",
    "        self._known_classes = self._total_classes\n",
    "        logging.info(\"Exemplar size: {}\".format(self.exemplar_size))\n",
    "\n",
    "    def incremental_train(self, data_manager):\n",
    "        self._cur_task += 1\n",
    "        self._total_classes = self._known_classes + data_manager.get_task_size(\n",
    "            self._cur_task\n",
    "        )\n",
    "        self._network.update_fc(self._total_classes)\n",
    "        logging.info(\n",
    "            \"Learning on {}-{}\".format(self._known_classes, self._total_classes)\n",
    "        )\n",
    "\n",
    "        train_dataset = data_manager.get_dataset(\n",
    "            np.arange(self._known_classes, self._total_classes),\n",
    "            source=\"train\",\n",
    "            mode=\"train\",\n",
    "            appendent=self._get_memory(),\n",
    "        )\n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers\n",
    "        )\n",
    "        test_dataset = data_manager.get_dataset(\n",
    "            np.arange(0, self._total_classes), source=\"test\", mode=\"test\"\n",
    "        )\n",
    "        self.test_loader = DataLoader(\n",
    "            test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers\n",
    "        )\n",
    "\n",
    "        if len(self._multiple_gpus) > 1:\n",
    "            self._network = nn.DataParallel(self._network, self._multiple_gpus)\n",
    "        self._train(self.train_loader, self.test_loader)\n",
    "        self.build_rehearsal_memory(data_manager, self.samples_per_class)\n",
    "        if len(self._multiple_gpus) > 1:\n",
    "            self._network = self._network.module\n",
    "\n",
    "    def _train(self, train_loader, test_loader):\n",
    "        self._network.to(self._device)\n",
    "        if self._old_network is not None:\n",
    "            self._old_network.to(self._device)\n",
    "\n",
    "        if self._cur_task == 0:\n",
    "            optimizer = optim.SGD(\n",
    "                self._network.parameters(),\n",
    "                momentum=0.9,\n",
    "                lr=init_lr,\n",
    "                weight_decay=init_weight_decay,\n",
    "            )\n",
    "            scheduler = optim.lr_scheduler.MultiStepLR(\n",
    "                optimizer=optimizer, milestones=init_milestones, gamma=init_lr_decay\n",
    "            )\n",
    "            self._init_train(train_loader, test_loader, optimizer, scheduler)\n",
    "        else:\n",
    "            optimizer = optim.SGD(\n",
    "                self._network.parameters(),\n",
    "                lr=lrate,\n",
    "                momentum=0.9,\n",
    "                weight_decay=weight_decay,\n",
    "            )  # 1e-5\n",
    "            scheduler = optim.lr_scheduler.MultiStepLR(\n",
    "                optimizer=optimizer, milestones=milestones, gamma=lrate_decay\n",
    "            )\n",
    "            self._update_representation(train_loader, test_loader, optimizer, scheduler)\n",
    "\n",
    "    def _init_train(self, train_loader, test_loader, optimizer, scheduler):\n",
    "        prog_bar = tqdm(range(init_epoch))\n",
    "        for _, epoch in enumerate(prog_bar):\n",
    "\n",
    "            if epoch == 150:  # Freeze layers after 100 epochs\n",
    "                self._freeze_except_last_two()\n",
    "\n",
    "                # Reinitialize optimizer to exclude frozen parameters\n",
    "                optimizer = optim.SGD(\n",
    "                    filter(lambda p: p.requires_grad, self._network.parameters()),\n",
    "                    lr=lrate,\n",
    "                    momentum=0.9,\n",
    "                    weight_decay=weight_decay,\n",
    "                )\n",
    "                scheduler = optim.lr_scheduler.MultiStepLR(\n",
    "                    optimizer=optimizer, milestones=milestones, gamma=lrate_decay\n",
    "                )\n",
    "            \n",
    "            self._network.train()\n",
    "            losses = 0.0\n",
    "            correct, total = 0, 0\n",
    "            for i, (_, inputs, targets) in enumerate(train_loader):\n",
    "                inputs, targets = inputs.to(self._device), targets.to(self._device)\n",
    "                logits = self._network(inputs)[\"logits\"]\n",
    "\n",
    "                loss = F.cross_entropy(logits, targets)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                losses += loss.item()\n",
    "\n",
    "                _, preds = torch.max(logits, dim=1)\n",
    "                correct += preds.eq(targets.expand_as(preds)).cpu().sum()\n",
    "                total += len(targets)\n",
    "\n",
    "            scheduler.step()\n",
    "            train_acc = np.around(tensor2numpy(correct) * 100 / total, decimals=2)\n",
    "\n",
    "            if epoch % 5 == 0:\n",
    "                test_acc = self._compute_accuracy(self._network, test_loader)\n",
    "                info = \"Task {}, Epoch {}/{} => Loss {:.3f}, Train_accy {:.2f}, Test_accy {:.2f}\".format(\n",
    "                    self._cur_task,\n",
    "                    epoch + 1,\n",
    "                    init_epoch,\n",
    "                    losses / len(train_loader),\n",
    "                    train_acc,\n",
    "                    test_acc,\n",
    "                )\n",
    "            else:\n",
    "                info = \"Task {}, Epoch {}/{} => Loss {:.3f}, Train_accy {:.2f}\".format(\n",
    "                    self._cur_task,\n",
    "                    epoch + 1,\n",
    "                    init_epoch,\n",
    "                    losses / len(train_loader),\n",
    "                    train_acc,\n",
    "                )\n",
    "\n",
    "            prog_bar.set_description(info)\n",
    "\n",
    "        logging.info(info)\n",
    "\n",
    "    def _update_representation(self, train_loader, test_loader, optimizer, scheduler):\n",
    "        prog_bar = tqdm(range(epochs))\n",
    "        for _, epoch in enumerate(prog_bar):\n",
    "\n",
    "            if epoch == 150:  # Freeze layers after 100 epochs\n",
    "                self._freeze_except_last_two()\n",
    "                # Reinitialize optimizer to exclude frozen parameters\n",
    "                optimizer = optim.SGD(\n",
    "                    filter(lambda p: p.requires_grad, self._network.parameters()),\n",
    "                    lr=lrate,\n",
    "                    momentum=0.9,\n",
    "                    weight_decay=weight_decay,\n",
    "                )\n",
    "                scheduler = optim.lr_scheduler.MultiStepLR(\n",
    "                    optimizer=optimizer, milestones=milestones, gamma=lrate_decay\n",
    "                )\n",
    "            \n",
    "            self._network.train()\n",
    "            losses = 0.0\n",
    "            correct, total = 0, 0\n",
    "            for i, (_, inputs, targets) in enumerate(train_loader):\n",
    "                inputs, targets = inputs.to(self._device), targets.to(self._device)\n",
    "                logits = self._network(inputs)[\"logits\"]\n",
    "\n",
    "                loss_clf = F.cross_entropy(logits, targets)\n",
    "                loss_kd = _KD_loss(\n",
    "                    logits[:, : self._known_classes],\n",
    "                    self._old_network(inputs)[\"logits\"],\n",
    "                    T,\n",
    "                )\n",
    "\n",
    "                loss = loss_clf + loss_kd\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                losses += loss.item()\n",
    "\n",
    "                _, preds = torch.max(logits, dim=1)\n",
    "                correct += preds.eq(targets.expand_as(preds)).cpu().sum()\n",
    "                total += len(targets)\n",
    "\n",
    "            scheduler.step()\n",
    "            train_acc = np.around(tensor2numpy(correct) * 100 / total, decimals=2)\n",
    "            if epoch % 5 == 0:\n",
    "                test_acc = self._compute_accuracy(self._network, test_loader)\n",
    "                info = \"Task {}, Epoch {}/{} => Loss {:.3f}, Train_accy {:.2f}, Test_accy {:.2f}\".format(\n",
    "                    self._cur_task,\n",
    "                    epoch + 1,\n",
    "                    epochs,\n",
    "                    losses / len(train_loader),\n",
    "                    train_acc,\n",
    "                    test_acc,\n",
    "                )\n",
    "            else:\n",
    "                info = \"Task {}, Epoch {}/{} => Loss {:.3f}, Train_accy {:.2f}\".format(\n",
    "                    self._cur_task,\n",
    "                    epoch + 1,\n",
    "                    epochs,\n",
    "                    losses / len(train_loader),\n",
    "                    train_acc,\n",
    "                )\n",
    "            prog_bar.set_description(info)\n",
    "        logging.info(info)\n",
    "\n",
    "    def _freeze_except_last_two(self):\n",
    "        # Freeze all layers except the last two\n",
    "        layer_count = len(list(self._network.parameters()))\n",
    "        for i, param in enumerate(self._network.parameters()):\n",
    "            if i < layer_count - 2:  # Freeze all layers except the last two\n",
    "                param.requires_grad = False\n",
    "            else:\n",
    "                param.requires_grad = True\n",
    "        logging.info(\"Frozen all layers except the last two.\")\n",
    "\n",
    "\n",
    "\n",
    "def _KD_loss(pred, soft, T):\n",
    "    pred = torch.log_softmax(pred / T, dim=1)\n",
    "    soft = torch.softmax(soft / T, dim=1)\n",
    "    return -1 * torch.mul(soft, pred).sum() / pred.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f497f290",
   "metadata": {
    "id": "E_5yol_l6epX",
    "papermill": {
     "duration": 0.005825,
     "end_time": "2024-12-30T15:22:34.291410",
     "exception": false,
     "start_time": "2024-12-30T15:22:34.285585",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bc542de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T15:22:34.304126Z",
     "iopub.status.busy": "2024-12-30T15:22:34.303855Z",
     "iopub.status.idle": "2024-12-30T15:22:34.938548Z",
     "shell.execute_reply": "2024-12-30T15:22:34.937895Z"
    },
    "id": "o9g5VMEg6epX",
    "papermill": {
     "duration": 0.643375,
     "end_time": "2024-12-30T15:22:34.940433",
     "exception": false,
     "start_time": "2024-12-30T15:22:34.297058",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import tarfile\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.datasets.folder import default_loader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import logging\n",
    "import torchvision\n",
    "\n",
    "class iData(object):\n",
    "    train_trsf = []\n",
    "    test_trsf = []\n",
    "    common_trsf = []\n",
    "    class_order = None\n",
    "\n",
    "\n",
    "class iCIFAR10(iData):\n",
    "    use_path = False\n",
    "    train_trsf = [\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.ColorJitter(brightness=63 / 255),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    "    test_trsf = [transforms.ToTensor()]\n",
    "    common_trsf = [\n",
    "        transforms.Normalize(\n",
    "            mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010)\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    class_order = np.arange(10).tolist()\n",
    "\n",
    "    def download_data(self):\n",
    "        train_dataset = datasets.cifar.CIFAR10(\"./data\", train=True, download=True)\n",
    "        test_dataset = datasets.cifar.CIFAR10(\"./data\", train=False, download=True)\n",
    "        self.train_data, self.train_targets = train_dataset.data, np.array(\n",
    "            train_dataset.targets\n",
    "        )\n",
    "        self.test_data, self.test_targets = test_dataset.data, np.array(\n",
    "            test_dataset.targets\n",
    "        )\n",
    "\n",
    "\n",
    "# class iCIFAR100(iData):\n",
    "#     use_path = False\n",
    "#     train_trsf = [\n",
    "#         transforms.RandomCrop(32, padding=4),\n",
    "#         transforms.RandomHorizontalFlip(),\n",
    "#         transforms.ColorJitter(brightness=63 / 255),\n",
    "#         transforms.ToTensor()\n",
    "#     ]\n",
    "#     test_trsf = [transforms.ToTensor()]\n",
    "#     common_trsf = [\n",
    "#         transforms.Normalize(\n",
    "#             mean=(0.5071, 0.4867, 0.4408), std=(0.2675, 0.2565, 0.2761)\n",
    "#         ),\n",
    "#     ]\n",
    "\n",
    "#     class_order = np.arange(100).tolist()\n",
    "\n",
    "#     def download_data(self):\n",
    "#         train_dataset = datasets.cifar.CIFAR100(\"./data\", train=True, download=True)\n",
    "#         test_dataset = datasets.cifar.CIFAR100(\"./data\", train=False, download=True)\n",
    "#         self.train_data, self.train_targets = train_dataset.data, np.array(\n",
    "#             train_dataset.targets\n",
    "#         )\n",
    "#         self.test_data, self.test_targets = test_dataset.data, np.array(\n",
    "#             test_dataset.targets\n",
    "#         )\n",
    "\n",
    "class iCIFAR100(iData):\n",
    "    use_path = False\n",
    "    train_trsf = transforms.Compose([\n",
    "        transforms.Resize((224, 224)), \n",
    "        transforms.RandomCrop(224, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(brightness=63 / 255),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.5071, 0.4867, 0.4408), std=(0.2675, 0.2565, 0.2761))\n",
    "    ])\n",
    "\n",
    "    test_trsf = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.5071, 0.4867, 0.4408), std=(0.2675, 0.2565, 0.2761))\n",
    "    ])\n",
    "\n",
    "    class_order = np.arange(100).tolist()\n",
    "\n",
    "    def download_data(self):\n",
    "        torchvision.datasets.CIFAR100.url = \"https://data.brainchip.com/dataset-mirror/cifar100/cifar-100-python.tar.gz\"\n",
    "\n",
    "        train_dataset = datasets.cifar.CIFAR100(\n",
    "            \"./data\", train=True, download=True, transform=self.train_trsf\n",
    "        )\n",
    "        test_dataset = datasets.cifar.CIFAR100(\n",
    "            \"./data\", train=False, download=True, transform=self.test_trsf\n",
    "        )\n",
    "        self.train_data, self.train_targets = train_dataset.data, np.array(\n",
    "            train_dataset.targets\n",
    "        )\n",
    "        self.test_data, self.test_targets = test_dataset.data, np.array(\n",
    "            test_dataset.targets\n",
    "        )\n",
    "\n",
    "\n",
    "class iImageNet1000(iData):\n",
    "    use_path = True\n",
    "    train_trsf = [\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(brightness=63 / 255),\n",
    "    ]\n",
    "    test_trsf = [\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "    ]\n",
    "    common_trsf = [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    "\n",
    "    class_order = np.arange(1000).tolist()\n",
    "\n",
    "    def download_data(self):\n",
    "        assert 0, \"You should specify the folder of your dataset\"\n",
    "        train_dir = \"[DATA-PATH]/train/\"\n",
    "        test_dir = \"[DATA-PATH]/val/\"\n",
    "\n",
    "        train_dset = datasets.ImageFolder(train_dir)\n",
    "        test_dset = datasets.ImageFolder(test_dir)\n",
    "\n",
    "        self.train_data, self.train_targets = split_images_labels(train_dset.imgs)\n",
    "        self.test_data, self.test_targets = split_images_labels(test_dset.imgs)\n",
    "\n",
    "class iCUB200(iData):\n",
    "    use_path = False\n",
    "    train_trsf = [\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "    ]\n",
    "    test_trsf = [\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "    ]\n",
    "    common_trsf = [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    "\n",
    "    class_order = np.arange(100).tolist()  # Assuming there are 200 classes in the CUB dataset\n",
    "\n",
    "    def __init__(self, root=None, download=True):\n",
    "        self.root = \"./CUB\"\n",
    "        self.base_folder = 'CUB_200_2011/images'\n",
    "        self.filename = 'CUB_200_2011.tgz'\n",
    "        self.data_folder = 'CUB_200_2011'\n",
    "        self.loader = default_loader\n",
    "        self.train_data = []\n",
    "        self.train_targets = []\n",
    "        self.test_data = []\n",
    "        self.test_targets = []\n",
    "\n",
    "        if download:\n",
    "            self.download_data()\n",
    "\n",
    "    def download_data(self):\n",
    "        dataset_path = os.path.join(\"/kaggle/input/cub-200/\", 'CUB_200_2011.tgz')\n",
    "        extract_path = os.path.join(self.root, self.data_folder)\n",
    "\n",
    "        if not os.path.exists(extract_path):\n",
    "            with tarfile.open(dataset_path, \"r:gz\") as tar:\n",
    "                tar.extractall(path=self.root)\n",
    "\n",
    "        # Load metadata\n",
    "        images = pd.read_csv(os.path.join(self.root, self.data_folder, 'images.txt'), sep=' ', names=['img_id', 'filepath'])\n",
    "        image_class_labels = pd.read_csv(os.path.join(self.root, self.data_folder, 'image_class_labels.txt'), sep=' ', names=['img_id', 'target'])\n",
    "        train_test_split = pd.read_csv(os.path.join(self.root, self.data_folder, 'train_test_split.txt'), sep=' ', names=['img_id', 'is_training_img'])\n",
    "\n",
    "        # Merge metadata\n",
    "        data = images.merge(image_class_labels, on='img_id').merge(train_test_split, on='img_id')\n",
    "\n",
    "        data = data[(data['target'] >= 0) & (data['target'] < 200)]\n",
    "\n",
    "        # Separate into train and test sets\n",
    "        train_data = data[data.is_training_img == 1]\n",
    "        test_data = data[data.is_training_img == 0]\n",
    "\n",
    "        # Assign train and test data\n",
    "        self.train_data, self.train_targets = self._load_data(train_data)\n",
    "        self.test_data, self.test_targets = self._load_data(test_data)\n",
    "\n",
    "    def _load_data(self, data):\n",
    "        imgs = []\n",
    "        targets = []\n",
    "        for idx, row in data.iterrows():\n",
    "            filepath = os.path.join(self.root, self.base_folder, row.filepath)\n",
    "            img = self.loader(filepath)\n",
    "            img = img.resize((224, 224))  # Resize to a fixed size (224x224)\n",
    "            img = np.array(img)  # Convert PIL image to numpy array\n",
    "            imgs.append(img)\n",
    "            targets.append(row.target - 1)  # Adjust to 0-based indexing\n",
    "        return imgs, targets\n",
    "    \n",
    "class iScenes(iData):\n",
    "    use_path = False\n",
    "    train_trsf = [\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "    ]\n",
    "    test_trsf = [\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "    ]\n",
    "    common_trsf = [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    "\n",
    "    def __init__(self, root=None, train=True):\n",
    "        self.root = root if root else \"/kaggle/input/indoor-scenes-cvpr-2019\"\n",
    "        self.base_folder = 'indoorCVPR_09/Images'\n",
    "        self.annotations_folder = 'indoorCVPR_09annotations'\n",
    "        self.loader = default_loader\n",
    "        self.classes = sorted([d.name for d in os.scandir(os.path.join(self.root, self.base_folder)) if d.is_dir()])\n",
    "\n",
    "        # Initialize data and targets for train and test sets\n",
    "        self.train_data = []\n",
    "        self.train_targets = []\n",
    "        self.test_data = []\n",
    "        self.test_targets = []\n",
    "\n",
    "        # Load train and test data\n",
    "        self.load_data(train=True)\n",
    "        self.load_data(train=False)\n",
    "        \n",
    "    def download_data(self):\n",
    "        return None\n",
    "\n",
    "    def load_data(self, train=True):\n",
    "        # Define paths for train and test sets\n",
    "        split_file = 'TrainImages.txt' if train else 'TestImages.txt'\n",
    "        split_path = os.path.join(self.root, split_file)\n",
    "\n",
    "        data = []\n",
    "        targets = []\n",
    "\n",
    "        with open(split_path, 'r') as f:\n",
    "            for line in f:\n",
    "                class_name, filename = line.strip().split('/')\n",
    "                label = self.classes.index(class_name)\n",
    "                filepath = os.path.join(self.root, self.base_folder, class_name, filename)\n",
    "                \n",
    "                img = self.loader(filepath)\n",
    "                img = img.resize((224, 224))  # Resize to match transformations\n",
    "                img = np.array(img)  # Convert to numpy array\n",
    "                \n",
    "                data.append(img)\n",
    "                targets.append(label)\n",
    "\n",
    "        # Assign loaded data to train or test attributes\n",
    "        if train:\n",
    "            self.train_data, self.train_targets = data, targets\n",
    "        else:\n",
    "            self.test_data, self.test_targets = data, targets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c652c34",
   "metadata": {
    "id": "rb5jwB_d6epX",
    "papermill": {
     "duration": 0.005593,
     "end_time": "2024-12-30T15:22:34.952278",
     "exception": false,
     "start_time": "2024-12-30T15:22:34.946685",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b78ec3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T15:22:34.965190Z",
     "iopub.status.busy": "2024-12-30T15:22:34.964800Z",
     "iopub.status.idle": "2024-12-30T15:22:34.998886Z",
     "shell.execute_reply": "2024-12-30T15:22:34.998326Z"
    },
    "id": "iPt3mD2s6epX",
    "papermill": {
     "duration": 0.042417,
     "end_time": "2024-12-30T15:22:35.000430",
     "exception": false,
     "start_time": "2024-12-30T15:22:34.958013",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "class DataManager(object):\n",
    "    def __init__(self, dataset_name, shuffle, seed, init_cls, increment):\n",
    "        self.dataset_name = dataset_name\n",
    "        self._setup_data(dataset_name, shuffle, seed)\n",
    "        assert init_cls <= len(self._class_order), \"No enough classes.\"\n",
    "        self._increments = [init_cls]\n",
    "        while sum(self._increments) + increment < len(self._class_order):\n",
    "            self._increments.append(increment)\n",
    "        offset = len(self._class_order) - sum(self._increments)\n",
    "        if offset > 0:\n",
    "            self._increments.append(offset)\n",
    "\n",
    "    @property\n",
    "    def nb_tasks(self):\n",
    "        return len(self._increments)\n",
    "\n",
    "    def get_task_size(self, task):\n",
    "        return self._increments[task]\n",
    "\n",
    "    def get_accumulate_tasksize(self,task):\n",
    "        return sum(self._increments[:task+1])\n",
    "\n",
    "    def get_total_classnum(self):\n",
    "        return len(self._class_order)\n",
    "\n",
    "    def get_dataset(\n",
    "        self, indices, source, mode, appendent=None, ret_data=False, m_rate=None\n",
    "    ):\n",
    "        if source == \"train\":\n",
    "            x, y = self._train_data, self._train_targets\n",
    "        elif source == \"test\":\n",
    "            x, y = self._test_data, self._test_targets\n",
    "        else:\n",
    "            raise ValueError(\"Unknown data source {}.\".format(source))\n",
    "\n",
    "        if mode == \"train\":\n",
    "            trsf = transforms.Compose([*self._train_trsf, *self._common_trsf])\n",
    "        elif mode == \"flip\":\n",
    "            trsf = transforms.Compose(\n",
    "                [\n",
    "                    *self._test_trsf,\n",
    "                    transforms.RandomHorizontalFlip(p=1.0),\n",
    "                    *self._common_trsf,\n",
    "                ]\n",
    "            )\n",
    "        elif mode == \"test\":\n",
    "            trsf = transforms.Compose([*self._test_trsf, *self._common_trsf])\n",
    "        else:\n",
    "            raise ValueError(\"Unknown mode {}.\".format(mode))\n",
    "\n",
    "        data, targets = [], []\n",
    "        for idx in indices:\n",
    "            if m_rate is None:\n",
    "                class_data, class_targets = self._select(\n",
    "                    x, y, low_range=idx, high_range=idx + 1\n",
    "                )\n",
    "            else:\n",
    "                class_data, class_targets = self._select_rmm(\n",
    "                    x, y, low_range=idx, high_range=idx + 1, m_rate=m_rate\n",
    "                )\n",
    "            data.append(class_data)\n",
    "            targets.append(class_targets)\n",
    "\n",
    "        if appendent is not None and len(appendent) != 0:\n",
    "            appendent_data, appendent_targets = appendent\n",
    "            data.append(appendent_data)\n",
    "            targets.append(appendent_targets)\n",
    "\n",
    "        # logging.info(data)\n",
    "        data = np.concatenate(data)\n",
    "        targets = np.concatenate(targets)\n",
    "\n",
    "        if ret_data:\n",
    "            return data, targets, DummyDataset(data, targets, trsf, self.use_path)\n",
    "        else:\n",
    "            return DummyDataset(data, targets, trsf, self.use_path)\n",
    "\n",
    "\n",
    "    def get_finetune_dataset(self,known_classes,total_classes,source,mode,appendent,type=\"ratio\"):\n",
    "        if source == 'train':\n",
    "            x, y = self._train_data, self._train_targets\n",
    "        elif source == 'test':\n",
    "            x, y = self._test_data, self._test_targets\n",
    "        else:\n",
    "            raise ValueError('Unknown data source {}.'.format(source))\n",
    "\n",
    "        if mode == 'train':\n",
    "            trsf = transforms.Compose([*self._train_trsf, *self._common_trsf])\n",
    "        elif mode == 'test':\n",
    "            trsf = transforms.Compose([*self._test_trsf, *self._common_trsf])\n",
    "        else:\n",
    "            raise ValueError('Unknown mode {}.'.format(mode))\n",
    "        val_data = []\n",
    "        val_targets = []\n",
    "\n",
    "        old_num_tot = 0\n",
    "        appendent_data, appendent_targets = appendent\n",
    "\n",
    "        for idx in range(0, known_classes):\n",
    "            append_data, append_targets = self._select(appendent_data, appendent_targets,\n",
    "                                                       low_range=idx, high_range=idx+1)\n",
    "            num=len(append_data)\n",
    "            if num == 0:\n",
    "                continue\n",
    "            old_num_tot += num\n",
    "            val_data.append(append_data)\n",
    "            val_targets.append(append_targets)\n",
    "        if type == \"ratio\":\n",
    "            new_num_tot = int(old_num_tot*(total_classes-known_classes)/known_classes)\n",
    "        elif type == \"same\":\n",
    "            new_num_tot = old_num_tot\n",
    "        else:\n",
    "            assert 0, \"not implemented yet\"\n",
    "        new_num_average = int(new_num_tot/(total_classes-known_classes))\n",
    "        for idx in range(known_classes,total_classes):\n",
    "            class_data, class_targets = self._select(x, y, low_range=idx, high_range=idx+1)\n",
    "            val_indx = np.random.choice(len(class_data),new_num_average, replace=False)\n",
    "            val_data.append(class_data[val_indx])\n",
    "            val_targets.append(class_targets[val_indx])\n",
    "        val_data=np.concatenate(val_data)\n",
    "        val_targets = np.concatenate(val_targets)\n",
    "        return DummyDataset(val_data, val_targets, trsf, self.use_path)\n",
    "\n",
    "    def get_dataset_with_split(\n",
    "        self, indices, source, mode, appendent=None, val_samples_per_class=0\n",
    "    ):\n",
    "        if source == \"train\":\n",
    "            x, y = self._train_data, self._train_targets\n",
    "        elif source == \"test\":\n",
    "            x, y = self._test_data, self._test_targets\n",
    "        else:\n",
    "            raise ValueError(\"Unknown data source {}.\".format(source))\n",
    "\n",
    "        if mode == \"train\":\n",
    "            trsf = transforms.Compose([*self._train_trsf, *self._common_trsf])\n",
    "        elif mode == \"test\":\n",
    "            trsf = transforms.Compose([*self._test_trsf, *self._common_trsf])\n",
    "        else:\n",
    "            raise ValueError(\"Unknown mode {}.\".format(mode))\n",
    "\n",
    "        train_data, train_targets = [], []\n",
    "        val_data, val_targets = [], []\n",
    "        for idx in indices:\n",
    "            class_data, class_targets = self._select(\n",
    "                x, y, low_range=idx, high_range=idx + 1\n",
    "            )\n",
    "            val_indx = np.random.choice(\n",
    "                len(class_data), val_samples_per_class, replace=False\n",
    "            )\n",
    "            train_indx = list(set(np.arange(len(class_data))) - set(val_indx))\n",
    "            val_data.append(class_data[val_indx])\n",
    "            val_targets.append(class_targets[val_indx])\n",
    "            train_data.append(class_data[train_indx])\n",
    "            train_targets.append(class_targets[train_indx])\n",
    "\n",
    "        if appendent is not None:\n",
    "            appendent_data, appendent_targets = appendent\n",
    "            for idx in range(0, int(np.max(appendent_targets)) + 1):\n",
    "                append_data, append_targets = self._select(\n",
    "                    appendent_data, appendent_targets, low_range=idx, high_range=idx + 1\n",
    "                )\n",
    "                val_indx = np.random.choice(\n",
    "                    len(append_data), val_samples_per_class, replace=False\n",
    "                )\n",
    "                train_indx = list(set(np.arange(len(append_data))) - set(val_indx))\n",
    "                val_data.append(append_data[val_indx])\n",
    "                val_targets.append(append_targets[val_indx])\n",
    "                train_data.append(append_data[train_indx])\n",
    "                train_targets.append(append_targets[train_indx])\n",
    "\n",
    "        train_data, train_targets = np.concatenate(train_data), np.concatenate(\n",
    "            train_targets\n",
    "        )\n",
    "        val_data, val_targets = np.concatenate(val_data), np.concatenate(val_targets)\n",
    "\n",
    "        return DummyDataset(\n",
    "            train_data, train_targets, trsf, self.use_path\n",
    "        ), DummyDataset(val_data, val_targets, trsf, self.use_path)\n",
    "\n",
    "    def _setup_data(self, dataset_name, shuffle, seed):\n",
    "        idata = _get_idata(dataset_name)\n",
    "        idata.download_data()\n",
    "\n",
    "        # Data\n",
    "        self._train_data, self._train_targets = idata.train_data, idata.train_targets\n",
    "        self._test_data, self._test_targets = idata.test_data, idata.test_targets\n",
    "        self.use_path = idata.use_path\n",
    "\n",
    "        # Transforms\n",
    "        self._train_trsf = idata.train_trsf\n",
    "        self._test_trsf = idata.test_trsf\n",
    "        self._common_trsf = idata.common_trsf\n",
    "\n",
    "        # Order\n",
    "        order = [i for i in range(len(np.unique(self._train_targets)))]\n",
    "        if shuffle:\n",
    "            np.random.seed(seed)\n",
    "            order = np.random.permutation(len(order)).tolist()\n",
    "        else:\n",
    "            order = idata.class_order\n",
    "        self._class_order = order\n",
    "        logging.info(self._class_order)\n",
    "\n",
    "        # Map indices\n",
    "        self._train_targets = _map_new_class_index(\n",
    "            self._train_targets, self._class_order\n",
    "        )\n",
    "        self._test_targets = _map_new_class_index(self._test_targets, self._class_order)\n",
    "\n",
    "    def _select(self, x, y, low_range, high_range):\n",
    "        idxes = np.where(np.logical_and(y >= low_range, y < high_range))[0]\n",
    "\n",
    "        if isinstance(x,np.ndarray):\n",
    "            x_return = x[idxes]\n",
    "        else:\n",
    "            x_return = []\n",
    "            for id in idxes:\n",
    "                x_return.append(x[id])\n",
    "        return x_return, y[idxes]\n",
    "\n",
    "    def _select_rmm(self, x, y, low_range, high_range, m_rate):\n",
    "        assert m_rate is not None\n",
    "        if m_rate != 0:\n",
    "            idxes = np.where(np.logical_and(y >= low_range, y < high_range))[0]\n",
    "            selected_idxes = np.random.randint(\n",
    "                0, len(idxes), size=int((1 - m_rate) * len(idxes))\n",
    "            )\n",
    "            new_idxes = idxes[selected_idxes]\n",
    "            new_idxes = np.sort(new_idxes)\n",
    "        else:\n",
    "            new_idxes = np.where(np.logical_and(y >= low_range, y < high_range))[0]\n",
    "        return x[new_idxes], y[new_idxes]\n",
    "\n",
    "    def getlen(self, index):\n",
    "        y = self._train_targets\n",
    "        return np.sum(np.where(y == index))\n",
    "\n",
    "\n",
    "class DummyDataset(Dataset):\n",
    "    def __init__(self, images, labels, trsf, use_path=False):\n",
    "        assert len(images) == len(labels), \"Data size error!\"\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.trsf = trsf\n",
    "        self.use_path = use_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # logging.info(\"Debug\")y\n",
    "        if self.use_path:\n",
    "            image = self.trsf(pil_loader(self.images[idx]))\n",
    "        else:\n",
    "            image = self.trsf(Image.fromarray(self.images[idx]))\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return idx, image, label\n",
    "\n",
    "\n",
    "def _map_new_class_index(y, order):\n",
    "    return np.array(list(map(lambda x: order.index(x), y)))\n",
    "\n",
    "\n",
    "def _get_idata(dataset_name):\n",
    "    name = dataset_name.lower()\n",
    "    if name == \"cifar10\":\n",
    "        return iCIFAR10()\n",
    "    elif name == \"cifar100\":\n",
    "        return iCIFAR100()\n",
    "    elif name == \"imagenet1000\":\n",
    "        return iImageNet1000()\n",
    "    elif name == \"imagenet100\":\n",
    "        return iImageNet100()\n",
    "    elif name == \"custom\":\n",
    "        # return iCIFAR100()\n",
    "#         return iCUB200()\n",
    "        return iScenes()\n",
    "    else:\n",
    "        raise NotImplementedError(\"Unknown dataset {}.\".format(dataset_name))\n",
    "\n",
    "\n",
    "def pil_loader(path):\n",
    "    \"\"\"\n",
    "    Ref:\n",
    "    https://pytorch.org/docs/stable/_modules/torchvision/datasets/folder.html#ImageFolder\n",
    "    \"\"\"\n",
    "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    logging.info(\"Path check: %s\", path)\n",
    "    with open(path, \"rb\") as f:\n",
    "        img = Image.open(f)\n",
    "        return img.convert(\"RGB\")\n",
    "\n",
    "\n",
    "def accimage_loader(path):\n",
    "    \"\"\"\n",
    "    Ref:\n",
    "    https://pytorch.org/docs/stable/_modules/torchvision/datasets/folder.html#ImageFolder\n",
    "    accimage is an accelerated Image loader and preprocessor leveraging Intel IPP.\n",
    "    accimage is available on conda-forge.\n",
    "    \"\"\"\n",
    "    import accimage\n",
    "\n",
    "    try:\n",
    "        return accimage.Image(path)\n",
    "    except IOError:\n",
    "        # Potentially a decoding problem, fall back to PIL.Image\n",
    "        return pil_loader(path)\n",
    "\n",
    "\n",
    "def default_loader(path):\n",
    "    \"\"\"\n",
    "    Ref:\n",
    "    https://pytorch.org/docs/stable/_modules/torchvision/datasets/folder.html#ImageFolder\n",
    "    \"\"\"\n",
    "    from torchvision import get_image_backend\n",
    "\n",
    "    if get_image_backend() == \"accimage\":\n",
    "        return accimage_loader(path)\n",
    "    else:\n",
    "        return pil_loader(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fc50bf",
   "metadata": {
    "id": "L_hcCCTa6epY",
    "papermill": {
     "duration": 0.005606,
     "end_time": "2024-12-30T15:22:35.011786",
     "exception": false,
     "start_time": "2024-12-30T15:22:35.006180",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a92a119",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T15:22:35.025436Z",
     "iopub.status.busy": "2024-12-30T15:22:35.025184Z",
     "iopub.status.idle": "2024-12-30T15:22:35.048571Z",
     "shell.execute_reply": "2024-12-30T15:22:35.047991Z"
    },
    "id": "FupP-Ms-6epY",
    "papermill": {
     "duration": 0.031899,
     "end_time": "2024-12-30T15:22:35.050146",
     "exception": false,
     "start_time": "2024-12-30T15:22:35.018247",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "import copy\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def get_model(model_name, args):\n",
    "    name = model_name.lower()\n",
    "    if name == \"icarl\":\n",
    "        return iCaRL(args)\n",
    "    elif name == \"bic\":\n",
    "        return BiC(args)\n",
    "    elif name == \"podnet\":\n",
    "        return PODNet(args)\n",
    "    elif name == \"lwf\":\n",
    "        return LwF(args)\n",
    "    elif name == \"ewc\":\n",
    "        return EWC(args)\n",
    "    elif name == \"wa\":\n",
    "        return WA(args)\n",
    "    elif name == \"der\":\n",
    "        return DER(args)\n",
    "    elif name == \"finetune\":\n",
    "        return Finetune(args)\n",
    "    elif name == \"replay\":\n",
    "        return Replay(args)\n",
    "    elif name == \"gem\":\n",
    "        return GEM(args)\n",
    "    elif name == \"coil\":\n",
    "        return COIL(args)\n",
    "    elif name == \"foster\":\n",
    "        return FOSTER(args)\n",
    "    elif name == \"rmm-icarl\":\n",
    "        return RMM_iCaRL(args)\n",
    "    elif name == \"rmm-foster\":\n",
    "        return RMM_FOSTER(args)\n",
    "    elif name == \"fetril\":\n",
    "        return FeTrIL(args)\n",
    "    elif name == \"pass\":\n",
    "        return PASS(args)\n",
    "    elif name == \"il2a\":\n",
    "        return IL2A(args)\n",
    "    elif name == \"ssre\":\n",
    "        return SSRE(args)\n",
    "    elif name == \"memo\":\n",
    "        return MEMO(args)\n",
    "    elif name == \"beefiso\":\n",
    "        return BEEFISO(args)\n",
    "    elif name == \"simplecil\":\n",
    "        return SimpleCIL(args)\n",
    "    else:\n",
    "        assert 0\n",
    "\n",
    "\n",
    "\n",
    "def train(args):\n",
    "    seed_list = copy.deepcopy(args[\"seed\"])\n",
    "    device = [0]\n",
    "    print(\"device----------------\", device)\n",
    "\n",
    "    for seed in seed_list:\n",
    "        args[\"seed\"] = seed\n",
    "        args[\"device\"] = device\n",
    "        _train(args)\n",
    "\n",
    "\n",
    "def _train(args):\n",
    "\n",
    "    init_cls = 0 if args [\"init_cls\"] == args[\"increment\"] else args[\"init_cls\"]\n",
    "    logs_name = \"logs/{}/{}/{}/{}\".format(args[\"model_name\"],args[\"dataset\"], init_cls, args['increment'])\n",
    "\n",
    "    if not os.path.exists(logs_name):\n",
    "        os.makedirs(logs_name)\n",
    "\n",
    "    logfilename = \"logs/{}/{}/{}/{}/{}_{}_{}\".format(\n",
    "        args[\"model_name\"],\n",
    "        args[\"dataset\"],\n",
    "        init_cls,\n",
    "        args[\"increment\"],\n",
    "        args[\"prefix\"],\n",
    "        args[\"seed\"],\n",
    "        args[\"convnet_type\"],\n",
    "    )\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s [%(filename)s] => %(message)s\",\n",
    "        handlers=[\n",
    "            logging.FileHandler(filename=logfilename + \".log\"),\n",
    "            logging.StreamHandler(sys.stdout),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    _set_random()\n",
    "    _set_device(args)\n",
    "    print_args(args)\n",
    "    data_manager = DataManager(\n",
    "        args[\"dataset\"],\n",
    "        args[\"shuffle\"],\n",
    "        args[\"seed\"],\n",
    "        args[\"init_cls\"],\n",
    "        args[\"increment\"],\n",
    "    )\n",
    "    model = get_model(args[\"model_name\"], args)\n",
    "\n",
    "    cnn_curve, nme_curve = {\"top1\": [], \"top5\": []}, {\"top1\": [], \"top5\": []}\n",
    "    cnn_matrix, nme_matrix = [], []\n",
    "\n",
    "    for task in range(data_manager.nb_tasks):\n",
    "        logging.info(\"All params: {}\".format(count_parameters(model._network)))\n",
    "        logging.info(\n",
    "            \"Trainable params: {}\".format(count_parameters(model._network, True))\n",
    "        )\n",
    "        model.incremental_train(data_manager)\n",
    "        cnn_accy, nme_accy = model.eval_task()\n",
    "        model.after_task()\n",
    "\n",
    "        if nme_accy is not None:\n",
    "            logging.info(\"CNN: {}\".format(cnn_accy[\"grouped\"]))\n",
    "            logging.info(\"NME: {}\".format(nme_accy[\"grouped\"]))\n",
    "\n",
    "            cnn_keys = [key for key in cnn_accy[\"grouped\"].keys() if '-' in key]\n",
    "            cnn_keys_sorted = sorted(cnn_keys)\n",
    "            cnn_values = [cnn_accy[\"grouped\"][key] for key in cnn_keys_sorted]\n",
    "            cnn_matrix.append(cnn_values)\n",
    "\n",
    "            nme_keys = [key for key in nme_accy[\"grouped\"].keys() if '-' in key]\n",
    "            nme_keys_sorted = sorted(nme_keys)\n",
    "            nme_values = [nme_accy[\"grouped\"][key] for key in nme_keys_sorted]\n",
    "            nme_matrix.append(nme_values)\n",
    "\n",
    "\n",
    "            cnn_curve[\"top1\"].append(cnn_accy[\"top1\"])\n",
    "            cnn_curve[\"top5\"].append(cnn_accy[\"top5\"])\n",
    "\n",
    "            nme_curve[\"top1\"].append(nme_accy[\"top1\"])\n",
    "            nme_curve[\"top5\"].append(nme_accy[\"top5\"])\n",
    "\n",
    "            logging.info(\"CNN top1 curve: {}\".format(cnn_curve[\"top1\"]))\n",
    "            logging.info(\"CNN top5 curve: {}\".format(cnn_curve[\"top5\"]))\n",
    "            logging.info(\"NME top1 curve: {}\".format(nme_curve[\"top1\"]))\n",
    "            logging.info(\"NME top5 curve: {}\\n\".format(nme_curve[\"top5\"]))\n",
    "\n",
    "            print('Average Accuracy (CNN):', sum(cnn_curve[\"top1\"])/len(cnn_curve[\"top1\"]))\n",
    "            print('Average Accuracy (NME):', sum(nme_curve[\"top1\"])/len(nme_curve[\"top1\"]))\n",
    "\n",
    "            logging.info(\"Average Accuracy (CNN): {}\".format(sum(cnn_curve[\"top1\"])/len(cnn_curve[\"top1\"])))\n",
    "            logging.info(\"Average Accuracy (NME): {}\".format(sum(nme_curve[\"top1\"])/len(nme_curve[\"top1\"])))\n",
    "        else:\n",
    "            logging.info(\"No NME accuracy.\")\n",
    "            logging.info(\"CNN: {}\".format(cnn_accy[\"grouped\"]))\n",
    "\n",
    "            cnn_keys = [key for key in cnn_accy[\"grouped\"].keys() if '-' in key]\n",
    "            cnn_keys_sorted = sorted(cnn_keys)\n",
    "            cnn_values = [cnn_accy[\"grouped\"][key] for key in cnn_keys_sorted]\n",
    "            cnn_matrix.append(cnn_values)\n",
    "\n",
    "            cnn_curve[\"top1\"].append(cnn_accy[\"top1\"])\n",
    "            cnn_curve[\"top5\"].append(cnn_accy[\"top5\"])\n",
    "\n",
    "            logging.info(\"CNN top1 curve: {}\".format(cnn_curve[\"top1\"]))\n",
    "            logging.info(\"CNN top5 curve: {}\\n\".format(cnn_curve[\"top5\"]))\n",
    "\n",
    "            print('Average Accuracy (CNN):', sum(cnn_curve[\"top1\"])/len(cnn_curve[\"top1\"]))\n",
    "            logging.info(\"Average Accuracy (CNN): {}\".format(sum(cnn_curve[\"top1\"])/len(cnn_curve[\"top1\"])))\n",
    "\n",
    "\n",
    "    if len(cnn_matrix)>0:\n",
    "        np_acctable = np.zeros([task + 1, task + 1])\n",
    "        for idxx, line in enumerate(cnn_matrix):\n",
    "            idxy = len(line)\n",
    "            np_acctable[idxx, :idxy] = np.array(line)\n",
    "        np_acctable = np_acctable.T\n",
    "        forgetting = np.mean((np.max(np_acctable, axis=1) - np_acctable[:, task])[:task])\n",
    "        print('Accuracy Matrix (CNN):')\n",
    "        print(np_acctable)\n",
    "        print('Forgetting (CNN):', forgetting)\n",
    "        logging.info('Forgetting (CNN): {}'.format(forgetting))\n",
    "    if len(nme_matrix)>0:\n",
    "        np_acctable = np.zeros([task + 1, task + 1])\n",
    "        for idxx, line in enumerate(nme_matrix):\n",
    "            idxy = len(line)\n",
    "            np_acctable[idxx, :idxy] = np.array(line)\n",
    "        np_acctable = np_acctable.T\n",
    "        forgetting = np.mean((np.max(np_acctable, axis=1) - np_acctable[:, task])[:task])\n",
    "        print('Accuracy Matrix (NME):')\n",
    "        print(np_acctable)\n",
    "        print('Forgetting (NME):', forgetting)\n",
    "        logging.info('Forgetting (NME):', forgetting)\n",
    "\n",
    "\n",
    "def _set_device(args):\n",
    "    device_type = args[\"device\"]\n",
    "\n",
    "    gpus = []\n",
    "\n",
    "    for device in device_type:\n",
    "        if device_type == -1:\n",
    "            device = torch.device(\"cpu\")\n",
    "            print('cpu selected')\n",
    "        else:\n",
    "            device = torch.device(\"cuda:{}\".format(device))\n",
    "            print('cuda selected')\n",
    "\n",
    "        gpus.append(device)\n",
    "\n",
    "    args[\"device\"] = gpus\n",
    "\n",
    "\n",
    "def _set_random():\n",
    "    torch.manual_seed(1)\n",
    "    torch.cuda.manual_seed(1)\n",
    "    torch.cuda.manual_seed_all(1)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def print_args(args):\n",
    "    for key, value in args.items():\n",
    "        logging.info(\"{}: {}\".format(key, value))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26585be8",
   "metadata": {
    "id": "W2UKo2Ua_fBg",
    "papermill": {
     "duration": 0.005417,
     "end_time": "2024-12-30T15:22:35.061255",
     "exception": false,
     "start_time": "2024-12-30T15:22:35.055838",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9764f83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T15:22:35.073208Z",
     "iopub.status.busy": "2024-12-30T15:22:35.072938Z",
     "iopub.status.idle": "2024-12-31T00:50:59.000095Z",
     "shell.execute_reply": "2024-12-31T00:50:58.998943Z"
    },
    "id": "o19VzUbi_hdK",
    "papermill": {
     "duration": 34103.935582,
     "end_time": "2024-12-31T00:50:59.002307",
     "exception": false,
     "start_time": "2024-12-30T15:22:35.066725",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device---------------- [0]\n",
      "cuda selected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "/tmp/ipykernel_23/1839517366.py:208: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(weights_path)\n",
      "Task 0, Epoch 150/150 => Loss 0.063, Train_accy 97.37: 100%|██████████| 150/150 [48:10<00:00, 19.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy (CNN): 93.1\n",
      "self.fc: \n",
      "\n",
      " SimpleLinear() \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 1, Epoch 200/200 => Loss 3.256, Train_accy 87.53: 100%|██████████| 200/200 [1:27:05<00:00, 26.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy (CNN): 86.94999999999999\n",
      "self.fc: \n",
      "\n",
      " SimpleLinear() \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 2, Epoch 200/200 => Loss 5.163, Train_accy 89.14: 100%|██████████| 200/200 [1:28:18<00:00, 26.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy (CNN): 82.57666666666665\n",
      "self.fc: \n",
      "\n",
      " SimpleLinear() \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 3, Epoch 200/200 => Loss 6.612, Train_accy 88.26: 100%|██████████| 200/200 [1:29:46<00:00, 26.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy (CNN): 77.45249999999999\n",
      "self.fc: \n",
      "\n",
      " SimpleLinear() \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 4, Epoch 200/200 => Loss 7.467, Train_accy 89.93: 100%|██████████| 200/200 [1:31:23<00:00, 27.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy (CNN): 73.27799999999999\n",
      "self.fc: \n",
      "\n",
      " SimpleLinear() \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 5, Epoch 200/200 => Loss 7.564, Train_accy 84.62: 100%|██████████| 200/200 [1:32:19<00:00, 27.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy (CNN): 70.395\n",
      "self.fc: \n",
      "\n",
      " SimpleLinear() \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 6, Epoch 200/200 => Loss 7.961, Train_accy 87.75: 100%|██████████| 200/200 [1:08:26<00:00, 20.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy (CNN): 67.93999999999998\n",
      "Accuracy Matrix (CNN):\n",
      "[[93.1  84.24 67.49 54.19 37.93 33.99 27.09]\n",
      " [ 0.   77.27 69.7  53.54 43.94 37.37 27.27]\n",
      " [ 0.    0.   84.42 61.81 55.28 52.26 44.72]\n",
      " [ 0.    0.    0.   78.89 67.84 64.82 61.31]\n",
      " [ 0.    0.    0.    0.   78.57 74.49 67.86]\n",
      " [ 0.    0.    0.    0.    0.   73.5  69.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.   84.14]]\n",
      "Forgetting (CNN): 31.416666666666657\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Directly define your parameters in the notebook as a dictionary\n",
    "    args = {\n",
    "        \"prefix\": \"VGG_pretrained_KAN_Scenes_epoch =250,lr = 0.001_freezeMLP\",\n",
    "        \"dataset\": \"custom\",\n",
    "        \"memory_size\": 2000,\n",
    "        \"memory_per_class\": 30,\n",
    "        \"fixed_memory\": True,\n",
    "        \"shuffle\": True,\n",
    "        \"init_cls\": 10,\n",
    "        \"increment\": 10,\n",
    "        \"model_name\": \"lwf\",\n",
    "        \"convnet_type\": \"resnet32\",\n",
    "        \"device\": [\"0\", \"1\", \"2\", \"3\"],\n",
    "        \"seed\": [1993]\n",
    "    }\n",
    "\n",
    "    # Train with the provided arguments\n",
    "    train(args)\n",
    "\n",
    "# Call the main function to run the code\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 358221,
     "sourceId": 702372,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1059701,
     "sourceId": 1782442,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5920986,
     "sourceId": 9685894,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5973867,
     "sourceId": 9756361,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5974221,
     "sourceId": 9756837,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 34114.856749,
   "end_time": "2024-12-31T00:51:01.620577",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-30T15:22:26.763828",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
